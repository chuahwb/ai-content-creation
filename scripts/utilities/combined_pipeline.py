# -*- coding: utf-8 -*-
"""Combined AI Imagery Pipeline

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0zHjiH2GnYbQGYo-EHyvOQ0pvouy2i4
"""

# -*- coding: utf-8 -*-
"""
Combined AI Imagery Pipeline

This Colab notebook merges the functionalities of two previous scripts:
1.  user_input_and_image_eval: Handles user input, image analysis, and marketing strategy generation.
2.  creative_prompt_to_image_gen: Handles style guidance, creative concept development, prompt assembly, and image generation.

The combined pipeline aims to:
- Provide a seamless, end-to-end workflow for AI-powered social media content generation.
- Preserve all core features, logic, and prompt engineering details from the original scripts.
- Enhance usability with a consolidated setup, UI improvements (like creativity level input),
  and streamlined output management.
- Improve traceability with staged logging.
"""

# @title Step 0: Install Prerequisites
# !pip install openai pydantic instructor python-dotenv tenacity Pillow requests -q

# @title Step 1: Setup - Import Libraries, Mount Drive, Define Paths & Constants

# --- Standard Library Imports ---
import json
import os
import time
import traceback
import io
import base64
import pathlib
import datetime
import random
import re
from typing import List, Optional, Dict, Any, Tuple

# --- Third-party Library Imports ---
import ipywidgets as widgets
from IPython.display import display, clear_output, Image as IPImage
from PIL import Image
from dotenv import load_dotenv
from pydantic import BaseModel, Field, field_validator, ValidationError
import instructor
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
import requests # For downloading images from URL

# --- OpenAI Specific Imports ---
try:
    from openai import OpenAI, APIConnectionError, RateLimitError, APIStatusError
    from openai.types.chat import ChatCompletion, ChatCompletionMessageParam # Added for type hinting
    from openai.types.chat.chat_completion import ChatCompletionMessage # Added for type hinting
    from openai.types.images_response import ImagesResponse
except ImportError:
    print("ERROR: openai library not found or old version. Please install with `pip install -U openai`")
    OpenAI = None
    ChatCompletion = None
    ImagesResponse = None
    ChatCompletionMessageParam = None # type: ignore
    ChatCompletionMessage = None # type: ignore

print("Libraries imported.")
if not OpenAI:
    print("WARNING: OpenAI library import failed. API calls will not work.")
if not instructor:
    print("WARNING: instructor library import failed. Structured LLM calls will not work.")
if not BaseModel:
    print("WARNING: pydantic library import failed. Pydantic models cannot be defined.")

# --- Mount Google Drive ---
try:
    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)
    print("✅ Google Drive mounted successfully.")
    DRIVE_MOUNTED = True
except ImportError:
    print("⚠️ Google Colab 'drive' import failed. Drive mounting skipped. .env file and outputs might not be accessible as configured.")
    DRIVE_MOUNTED = False
except Exception as e:
    print(f"❌ An error occurred during Google Drive mounting: {e}")
    DRIVE_MOUNTED = False

# --- Define Base Paths ---
# It's recommended to place your .env file in a 'secrets' folder within this base path
DRIVE_BASE_PATH = '/content/drive/MyDrive/AI Imagery Marketing Tool/Colab Notebook' if DRIVE_MOUNTED else '.'
PIPELINE_OUTPUT_BASE_DIR = os.path.join(DRIVE_BASE_PATH, 'combined_pipeline_runs')

# Create base output directory if it doesn't exist
if not os.path.exists(PIPELINE_OUTPUT_BASE_DIR):
    try:
        os.makedirs(PIPELINE_OUTPUT_BASE_DIR, exist_ok=True)
        print(f"✅ Base output directory created/verified: {PIPELINE_OUTPUT_BASE_DIR}")
    except Exception as e:
        print(f"❌ Error creating base output directory {PIPELINE_OUTPUT_BASE_DIR}: {e}")
        PIPELINE_OUTPUT_BASE_DIR = '.' # Fallback to current directory

# --- Load API Keys from .env ---
dotenv_path = os.path.join(DRIVE_BASE_PATH, "colab_secrets/.env") # Standardized path
OPENROUTER_API_KEY = None
GEMINI_API_KEY_IMG_EVAL = None # For image evaluation if using Gemini
GEMINI_API_KEY_STRATEGY = None # For strategy if using Gemini
GEMINI_API_KEY_CREATIVE = None # For creative expert if using Gemini
OPENAI_API_KEY_FOR_IMAGE_GEN = None # Specifically for OpenAI's image generation

if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    print(f"✅ Loaded .env file from path: {dotenv_path}")
    OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
    GEMINI_API_KEY = os.getenv("GEMINI_API_KEY_1")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    GEMINI_API_KEY_IMG_EVAL = os.getenv("GEMINI_API_KEY_IMG_EVAL") # Example, adjust .env variable names
    GEMINI_API_KEY_STRATEGY = os.getenv("GEMINI_API_KEY_STRATEGY")
    GEMINI_API_KEY_CREATIVE = os.getenv("GEMINI_API_KEY_CREATIVE")
else:
    print(f"⚠️ Warning: .env file not found at {dotenv_path}. API keys should be set as environment variables or entered manually if prompted.")

# --- LLM Client Configuration ---
# Text LLM Clients (OpenRouter or Gemini)
base_llm_client_img_eval = None
instructor_client_img_eval = None
base_llm_client_strategy = None
instructor_client_strategy = None
base_llm_client_style_guide = None
instructor_client_style_guide = None
base_llm_client_creative_expert = None
instructor_client_creative_expert = None

# Image Generation Client (OpenAI)
image_gen_client = None

MAX_LLM_RETRIES = 1
FORCE_MANUAL_JSON_PARSE = False # Set to False to try Instructor first where applicable
VERBOSE_COST_LATENCY_SUMMARY = True # ADDED: Control verbosity of cost/latency summary

# --- Model Definitions ---
# Phase 1 Models
IMG_EVAL_MODEL_PROVIDER = "OpenRouter"  # "OpenRouter" or "Gemini"
IMG_EVAL_MODEL_ID = "openai/gpt-4.1-mini" # E.g., "openai/gpt-4-vision-preview", "google/gemini-pro-vision"

STRATEGY_MODEL_PROVIDER = "OpenRouter" # "OpenRouter" or "Gemini"
STRATEGY_MODEL_ID = "openai/gpt-4.1-mini"  # E.g., "openai/gpt-4-turbo", "google/gemini-1.5-pro-latest"

# Phase 2 Models
STYLE_GUIDER_MODEL_PROVIDER = "OpenRouter" # "OpenRouter" or "Gemini"
STYLE_GUIDER_MODEL_ID = "google/gemini-2.5-pro-preview" #"openai/o4-mini" "deepseek/deepseek-r1-0528" "qwen/qwen3-235b-a22b" "google/gemini-2.5-pro-preview"

CREATIVE_EXPERT_MODEL_PROVIDER = "OpenRouter" # "OpenRouter" or "Gemini"
CREATIVE_EXPERT_MODEL_ID = "google/gemini-2.5-pro-preview" #"openai/o4-mini" "deepseek/deepseek-r1-0528" "qwen/qwen3-235b-a22b" "google/gemini-2.5-pro-preview"

IMAGE_GENERATION_MODEL_ID = "gpt-image-1"

# Models known to have issues with instructor's default TOOLS mode via OpenRouter
INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS = ["openai/o4-mini","google/gemini-2.5-pro-preview"]

# --- Model Pricing (USD) ---
# Prices per 1 Million tokens for text, per image for image models
# Source: OpenRouter for text models, OpenAI for DALL-E 3 (as of June 2025 - placeholder)
# User should verify current pricing from respective providers.
MODEL_PRICING = {
    "openai/o4-mini": { # Used for Style Guider, Creative Expert
        "provider": "OpenRouter",
        "input_cost_per_mtok": 1.10,
        "output_cost_per_mtok": 4.40,
        "currency": "USD",
        "notes": "Pricing for openai/o4-mini via OpenRouter."
    },
    "openai/gpt-4.1-mini": { # Used for Image Eval, Niche ID, Strategy Gen
        "provider": "OpenRouter", # Assuming this is an OpenRouter model ID
        "input_cost_per_mtok": 0.40,
        "output_cost_per_mtok": 1.20,
        "currency": "USD",
        "notes": "Pricing for openai/gpt-4.1-mini via OpenRouter."
    },
    "deepseek/deepseek-r1-0528": { # Explicitly adding for reference if needed
        "provider": "OpenRouter",
        "input_cost_per_mtok": 0.50,
        "output_cost_per_mtok": 2.150,
        "currency": "USD",
        "notes": "Pricing for deepseek/deepseek-r1-0528 via OpenRouter."
    },
    "qwen/qwen3-235b-a22b": { # Explicitly adding for reference if needed
        "provider": "OpenRouter",
        "input_cost_per_mtok": 0.14,
        "output_cost_per_mtok": 2,
        "currency": "USD",
        "notes": "Pricing for qwen/qwen3-235b-a22b via OpenRouter."
    },
    "google/gemini-2.5-pro-preview": { # Added pricing for this model
        "provider": "OpenRouter", # Assuming OpenRouter access
        "input_cost_per_mtok": 1.25,  # Example: Gemini 1.5 Pro on OpenRouter
        "output_cost_per_mtok": 10.00, # Example: Gemini 1.5 Pro on OpenRouter
        "currency": "USD",
        "notes": "Pricing for google/gemini-2.5-pro-preview via OpenRouter (using Gemini 1.5 Pro rates as proxy)."
    },
    "gpt-image-1": {
        "provider": "OpenAI", # Assuming OpenAI as provider
        "currency": "USD",
        "cost_per_image": { # Direct per-image costs based on quality and resolution
            "low": {
                "1024x1024": 0.011,
                "1024x1536": 0.016,
                "1536x1024": 0.016
            },
            "medium": {
                "1024x1024": 0.042,
                "1024x1536": 0.063,
                "1536x1024": 0.063
            },
            "high": {
                "1024x1024": 0.167,
                "1024x1536": 0.250,
                "1536x1024": 0.250
            }
        },
        "input_text_cost_per_mtok": 10.00, # For prompt text tokens
        # "input_image_cached_cost_per_mtok": 2.50, # Not currently used in this script's logic
        # "output_image_tokens_cost_per_mtok": 40.00, # Using direct per-image cost instead
        "notes": "Pricing for gpt-image-1. Uses direct per-image cost based on quality/resolution, plus input text token cost. Input image token costs for edits are not explicitly calculated here."
    },
}

# Helper to configure a client
def configure_llm_client(provider_name: str, model_id: str, purpose: str = "text generation"):
    """Configures and returns a base OpenAI client and an instructor-patched client."""
    base_client = None
    instructor_client_patched = None
    api_key_to_use = None
    base_url_to_use = None

    if provider_name == "OpenRouter":
        if not OPENROUTER_API_KEY:
            print(f"⚠️ OpenRouter API Key not found for {purpose}. Client not configured.")
            return None, None
        api_key_to_use = OPENROUTER_API_KEY
        base_url_to_use = "https://openrouter.ai/api/v1"
        print(f"Configuring OpenRouter client for {purpose} with model {model_id}.")
    elif provider_name == "Gemini":
        if not GEMINI_API_KEY:
            print(f"⚠️ Gemini API Key not found for {purpose}. Client not configured.")
            return None, None
        api_key_to_use = GEMINI_API_KEY
        # Note: Gemini via OpenAI client library typically uses a specific base URL format
        # or might require google-generativeai library directly for native API.
        # This setup assumes an OpenAI-compatible endpoint for Gemini if using OpenAI client.
        # For direct Gemini, you'd use:
        # import google.generativeai as genai
        # genai.configure(api_key=gemini_key)
        # base_client = genai.GenerativeModel(model_id)
        # For this example, we'll stick to OpenAI client structure, assuming a compatible proxy or future support.
        # If using OpenRouter for Gemini models, provider_name should be "OpenRouter" and model_id "google/gemini..."
        base_url_to_use = "https://generativelanguage.googleapis.com/v1beta/openai/" # Placeholder, adjust if using direct Gemini with OpenAI client
        print(f"Configuring Gemini client (via OpenAI library structure) for {purpose} with model {model_id}.")
        print("NOTE: Direct Gemini integration might require 'google-generativeai' library and different client initialization.")
    else:
        print(f"Unsupported provider: {provider_name} for {purpose}.")
        return None, None

    if OpenAI and api_key_to_use:
        try:
            base_client = OpenAI(
                api_key=api_key_to_use,
                base_url=base_url_to_use,
                max_retries=MAX_LLM_RETRIES,
                # OpenRouter specific headers if needed
                # default_headers={
                #    "HTTP-Referer": "YOUR_SITE_URL", # Optional
                #    "X-Title": "YOUR_APP_NAME" # Optional
                # }
            )
            if instructor and not FORCE_MANUAL_JSON_PARSE: # Check global flag first
                instructor_client_patched = instructor.patch(base_client) # Use default mode patching
                print(f"✅ {provider_name} client for {purpose} configured (Instructor patched - default mode). Model: {model_id}")
            elif instructor and FORCE_MANUAL_JSON_PARSE:
                instructor_client_patched = base_client # Use base if forcing manual or instructor fails
                print(f"✅ {provider_name} client for {purpose} configured (Manual JSON parsing). Model: {model_id}")
            else: # Instructor not available or not used
                instructor_client_patched = base_client
                print(f"✅ {provider_name} client for {purpose} configured (Instructor not available/skipped). Model: {model_id}")

        except Exception as e:
            print(f"❌ Error initializing {provider_name} client for {purpose}: {e}")
            base_client = None
            instructor_client_patched = None
    return base_client, instructor_client_patched

# Configure clients for each purpose
base_llm_client_img_eval, instructor_client_img_eval = configure_llm_client(IMG_EVAL_MODEL_PROVIDER, IMG_EVAL_MODEL_ID, "Image Evaluation")
base_llm_client_strategy, instructor_client_strategy = configure_llm_client(STRATEGY_MODEL_PROVIDER, STRATEGY_MODEL_ID, "Strategy Generation")
base_llm_client_style_guide, instructor_client_style_guide = configure_llm_client(STYLE_GUIDER_MODEL_PROVIDER, STYLE_GUIDER_MODEL_ID, "Style Guidance") # Assuming creative key for style
base_llm_client_creative_expert, instructor_client_creative_expert = configure_llm_client(CREATIVE_EXPERT_MODEL_PROVIDER, CREATIVE_EXPERT_MODEL_ID, "Creative Expert")


# Configure Image Generation Client (typically OpenAI)
if OpenAI and OPENAI_API_KEY:
    try:
        image_gen_client = OpenAI(
            api_key=OPENAI_API_KEY,
            max_retries=MAX_LLM_RETRIES
        )
        print(f"✅ Image Generation client (OpenAI) configured. Model: {IMAGE_GENERATION_MODEL_ID}")
    except Exception as e:
        print(f"❌ Error initializing Image Generation client: {e}")
        image_gen_client = None
elif not OPENAI_API_KEY:
     print("⚠️ OPENAI_API_KEY_FOR_IMAGE_GEN not found. Image Generation client not configured.")
else:
     print("⚠️ OpenAI library not available. Image Generation client not configured.")


# --- Define Task Types, Platforms, and Marketing Pools ---
TASK_TYPES = [
    'Select Task...',
    '1. Product Photography',
    '2. Promotional Graphics & Announcements',
    '3. Store Atmosphere & Decor',
    '4. Menu Spotlights',
    '5. Cultural & Community Content',
    '6. Recipes & Food Tips',
    '7. Brand Story & Milestones',
    '8. Behind the Scenes Imagery'
]

SOCIAL_MEDIA_PLATFORMS = {
    'Select Platform...': None,
    'Instagram Post (1:1 Square)': {'width': 1080, 'height': 1080, 'aspect_ratio': '1:1'},
    'Instagram Story/Reel (9:16 Vertical)': {'width': 1080, 'height': 1920, 'aspect_ratio': '9:16'},
    'Facebook Post (Mixed)': {'width': 1200, 'height': 630, 'aspect_ratio': '1.91:1'}, # More common FB link preview
    'Pinterest Pin (2:3 Vertical)': {'width': 1024, 'height': 1536, 'aspect_ratio': '2:3'}, # Common Pinterest
    'Xiaohongshu (Red Note) (3:4 Vertical)': {'width': 1080, 'height': 1440, 'aspect_ratio': '3:4'},
}
PLATFORM_DISPLAY_NAMES = list(SOCIAL_MEDIA_PLATFORMS.keys())

TASK_GROUP_POOLS = {
    "product_focus": {
        "audience": ["Foodies/Bloggers", "Local Residents", "Health-Conscious Eaters", "Vegetarians/Vegans", "Budget-Conscious Diners", "Luxury Seekers", "Specific Dietary Needs (e.g., Gluten-Free, Nut-Free)", "Tourists/Visitors", "Young Professionals (25-35)", "Families with Children", "Online Order Customers", "Takeaway Customers", "Diners seeking specific cuisine"],
        "niche": ["Casual Dining", "Fine Dining", "Cafe/Coffee Shop", "Bakery/Patisserie", "Fast Food/QSR", "Ethnic Cuisine (e.g., Italian, Thai, Mexican, Japanese, Indian)", "Specialty (e.g., Vegan, Seafood, Steakhouse, Organic)", "Brunch Spot", "Dessert Place", "Takeaway/Delivery Focused", "Gourmet Burger Joint", "Artisan Pizza Place", "Healthy Bowls/Salads"],
        "objective": ["Create Appetite Appeal", "Showcase Quality/Freshness", "Promote Specific Menu Item", "Highlight Ingredients/Sourcing", "Increase Online Orders/Reservations", "Drive Trial of New Item", "Attract New Customers", "Generate User-Generated Content (e.g., encourage photos)", "Justify Price Point", "Visually Explain Complex Dish", "Feature Seasonal Specials", "Differentiate from Competitors"],
        "voice": ["Mouth-watering & Descriptive", "Sophisticated & Elegant", "Fresh & Vibrant", "Authentic & Honest", "Simple & Direct", "Playful & Fun", "Informative & Helpful", "Artistic & Aspirational", "Clean & Minimalist", "Rustic & Homely"]
    },
    "promotion": {
        "audience": ["Budget-Conscious Diners", "Local Residents", "Students", "Young Professionals (25-35)", "Families with Children", "Existing Customers", "New Customers", "Loyalty Program Members", "Social Media Followers", "Event Attendees (for future events)"],
        "niche": ["Casual Dining", "Cafe/Coffee Shop", "Bakery/Patisserie", "Fast Food/QSR", "Takeaway/Delivery Focused", "Family Restaurant", "Bar/Pub", "Pizza Place", "Burger Joint"],
        "objective": ["Drive Short-Term Sales", "Increase Foot Traffic (Weekday/Weekend)", "Boost Online Orders", "Announce Special Offer/Discount", "Promote Event (e.g., Live Music, Quiz Night)", "Create Urgency (Limited Time Offer)", "Drive Trial of New Item/Offer", "Reward Loyal Customers", "Attract New Customers", "Increase Average Order Value (e.g., combo deals)"],
        "voice": ["Urgent & Exciting", "Friendly & Casual", "Clear & Direct", "Playful & Fun", "Value-Oriented", "Exclusive (for loyalty)", "Benefit-Driven", "Enthusiastic & Bold"]
    },
    "brand_atmosphere": {
        "audience": ["Local Residents", "Tourists/Visitors", "Young Professionals (25-35)", "Couples", "Foodies/Bloggers", "Remote Workers", "Event Planners", "Potential Employees", "Community Groups", "Art/Music Lovers (if relevant)"],
        "niche": ["Casual Dining", "Fine Dining", "Cafe/Coffee Shop", "Bakery/Patisserie", "Bar/Pub", "Ethnic Cuisine", "Specialty", "Romantic Restaurant", "Community Hub", "Venue with View", "Pet-Friendly Spot"],
        "objective": ["Increase Brand Awareness", "Highlight Atmosphere/Ambience/Decor", "Build Community", "Showcase Brand Personality/Values", "Build Emotional Connection", "Increase Brand Loyalty", "Attract Talent (BTS)", "Generate Interest for Events/Bookings", "Show Authenticity/Transparency (BTS)", "Tell Founder's Story", "Celebrate Milestone", "Feature Staff/Team", "Highlight Local Partnerships/Sourcing"],
        "voice": ["Warm & Welcoming", "Cozy & Comforting", "Sophisticated & Elegant", "Trendy & Energetic", "Authentic & Honest", "Community-Focused", "Aspirational & Inspiring", "Behind-the-Scenes & Candid", "Passionate & Dedicated", "Nostalgic & Reflective (for story)"]
    },
    "informative": {
         "audience": ["Home Cooks", "Foodies/Bloggers", "Health-Conscious Eaters", "Families with Children", "Budget-Conscious Diners", "Specific Dietary Needs", "Cooking Enthusiasts", "Beginner Cooks"],
         "niche": ["General F&B", "Cafe/Coffee Shop", "Bakery/Patisserie", "Ethnic Cuisine", "Specialty (e.g., Vegan)", "Health Food Store", "Cooking School", "Restaurant Blog", "Ingredient Supplier"],
         "objective": ["Educate Customers", "Provide Value/Utility", "Establish Expertise/Authority", "Increase Engagement on Social Media", "Drive Website/Blog Traffic", "Promote Specific Ingredients/Products", "Build Community around Food/Cooking", "Encourage Home Cooking (with brand ingredients)", "Simplify Complex Techniques", "Inspire Creativity"],
         "voice": ["Informative & Helpful", "Authoritative & Expert", "Friendly & Approachable", "Inspiring & Creative", "Clear & Concise", "Authentic & Passionate", "Step-by-Step & Practical", "Encouraging & Supportive"]
    },
    "default": {
        "audience": ["Local Residents", "Young Professionals (25-35)", "Families with Children", "Foodies/Bloggers", "General Social Media Users", "Tourists/Visitors", "Budget-Conscious Diners", "Health-Conscious Eaters"],
        "niche": ["Casual Dining", "Cafe/Coffee Shop", "Takeaway/Delivery Focused", "Bakery/Patisserie", "Fast Food/QSR", "General F&B"],
        "objective": ["Increase Brand Awareness", "Drive Foot Traffic", "Increase Engagement on Social Media", "Attract New Customers", "Showcase Product/Service", "Create Appetite Appeal", "Build Community"],
        "voice": ["Friendly & Casual", "Warm & Welcoming", "Authentic & Honest", "Clear & Direct", "Engaging & Vibrant"]
    }
}
print("✅ Tasks, Platforms, and Marketing Pools defined.")

# @title Step 2: Define Pydantic Models (Consolidated)

if BaseModel:
    # --- Models from Upstream (user_input_and_image_eval) ---
    class ImageAnalysisResult(BaseModel):
        """Structured result of the objective visual analysis of the image."""
        main_subject: str = Field(..., description="The single, primary subject of the image (e.g., 'Gourmet Burger', 'Latte Art', 'Restaurant Interior'). Should be concise and based SOLELY on visual content.")
        secondary_elements: Optional[List[str]] = Field(None, description="List other notable objects or elements visually present in the image. Omit if not applicable or if analysis is focused only on main_subject.")
        setting_environment: Optional[str] = Field(None, description="Describe the background or setting of the image as visually observed. Omit if not applicable or if analysis is focused only on main_subject.")
        style_mood: Optional[str] = Field(None, description="Describe the inferred visual style, mood, or atmosphere of the image based SOLELY on its visual content. Omit if not applicable or if analysis is focused only on main_subject.")
        extracted_text: Optional[str] = Field(None, description="Extract any visible text from the image. Omit if no text is visible or if analysis is focused only on main_subject.")

    class RelevantNicheList(BaseModel):
        """Identifies a list of relevant F&B niches for the given context."""
        relevant_niches: List[str] = Field(..., description="A list of 3-5 diverse but highly relevant F&B niches based on the input context (e.g., ['Ethnic Cuisine (Thai)', 'Casual Dining', 'Takeaway/Delivery Focused']). Prioritize niches directly related to the image subject or task description.")

    class MarketingGoalSetStage2(BaseModel):
        """Represents a set of marketing goals (audience, objective, voice) aligned with a specific niche."""
        target_audience: str = Field(..., description="Specific target audience group, generated based on context and relevant to the predetermined niche.")
        target_objective: str = Field(..., description="The primary marketing objective for this asset, generated based on context and relevant to the predetermined niche/task.")
        target_voice: str = Field(..., description="The desired brand voice or tone, generated based on context and relevant to the predetermined niche/audience.")

    class MarketingStrategyOutputStage2(BaseModel):
         """Container for N suggested marketing goal combinations (audience, objective, voice)."""
         strategies: List[MarketingGoalSetStage2] = Field(..., description="A list of N diverse and strategically sound marketing goal combinations (audience, objective, voice) aligned with a predetermined niche.")

    # This is the final structure for a strategy used throughout the pipeline
    class MarketingGoalSetFinal(BaseModel):
        """Represents a complete set of marketing goals for a creative direction."""
        target_audience: str
        target_niche: str
        target_objective: str
        target_voice: str

    # --- Models from Downstream (creative_prompt_to_image_gen) ---
    class VisualConceptDetails(BaseModel):
        """Detailed breakdown of the visual concept."""
        main_subject: Optional[str] = Field(None, description="Detailed description of the primary subject(s) and their interaction within the scene. This field should be omitted (set to null) if a reference image is provided and the default behavior (use reference subject) is intended.")
        composition_and_framing: str = Field(..., description="Description of the composition, camera angle, shot type (e.g., close-up, wide shot), and framing.")
        background_environment: str = Field(..., description="Description of the background, setting, or environment.")
        foreground_elements: Optional[str] = Field(None, description="Description of any significant foreground elements.")
        lighting_and_mood: str = Field(..., description="Description of the lighting style (e.g., natural, studio, dramatic) and the overall mood or atmosphere.")
        color_palette: str = Field(..., description="Description of the key colors, color harmony (e.g., analogous, complementary), and overall color tone.")
        visual_style: str = Field(..., description="Description of the artistic or visual style (e.g., photorealistic, illustration, graphic design, vintage). This should include key style descriptors and be highly creative if user input is minimal.")
        promotional_text_visuals: Optional[str] = Field(None, description="Description of how promotional text (from task_description) should be visualized, including content, style, font characteristics, and placement suggestions. Omit (set to null) if user input 'render_text' is false.")
        branding_visuals: Optional[str] = Field(None, description="Description of how branding elements (logo placeholders, taglines, specific brand fonts/colors mentioned in branding_elements input) should be visually incorporated. Omit (set to null) if user input 'apply_branding' is false.")
        texture_and_details: Optional[str] = Field(None, description="Specific notes on textures, materials, or fine details.")
        negative_elements: Optional[str] = Field(None, description="Specific elements or concepts to actively avoid in the image.")
        creative_reasoning: Optional[str] = Field(None, description="Brief explanation connecting the key visual choices (style, mood, composition, subject focus) back to the marketing strategy (audience, niche, objective, voice) and user inputs.")
        suggested_alt_text: str = Field(..., description="Concise, descriptive alt text (100-125 characters) for SEO and accessibility. Should clearly describe the image's subject, setting, and any important actions or text, naturally incorporating primary keywords from the marketing strategy.")

    class StyleGuidance(BaseModel):
        """Defines a specific style direction for a visual concept."""
        style_keywords: List[str] = Field(..., description="A list of 3-5 concise keywords defining the core visual style (e.g., 'photorealistic', 'impressionist', 'surreal').")
        style_description: str = Field(..., description="A detailed (2-3 sentence) description elaborating on the feeling, key characteristics, and specific artistic references or constraints of the style suggested by the keywords.")
        marketing_impact: str = Field(..., description="A brief explanation of how this style supports social media marketing goals (e.g., shareability, engagement, brand recall) for the target audience and platform.")
        source_strategy_index: int = Field(..., description="The index of the marketing strategy this style guidance corresponds to.")

    class StyleGuidanceList(BaseModel):
        """A list of style guidance sets, one for each marketing strategy."""
        style_guidance_sets: List[StyleGuidance]

    class ImageGenerationPrompt(BaseModel):
        """
        Structured prompt details generated by the Creative Expert agent,
        containing a breakdown of the visual concept. This structure will be
        processed later to create the final prompt for the text-to-image model.
        """
        visual_concept: VisualConceptDetails = Field(..., description="The detailed, structured breakdown of the visual concept.")
        # aspect_ratio is handled during prompt assembly and image generation call, not stored here directly
        source_strategy_index: Optional[int] = Field(None, description="Index linking back to the source marketing strategy in the input JSON.")

    class CostDetail(BaseModel):
        """Details of cost for a specific pipeline stage or model call."""
        stage_name: str
        model_id: str
        provider: Optional[str] = None
        duration_seconds: Optional[float] = None
        input_tokens: Optional[int] = None
        output_tokens: Optional[int] = None
        input_cost_usd: Optional[float] = None
        output_cost_usd: Optional[float] = None
        # For images
        images_generated: Optional[int] = None
        resolution: Optional[str] = None
        quality: Optional[str] = None
        image_cost_usd: Optional[float] = None
        total_stage_cost_usd: float = 0.0
        cost_calculation_notes: Optional[str] = None

    class PipelineCostSummary(BaseModel):
        """Overall cost summary for the pipeline run."""
        stage_costs: List[CostDetail] = []
        total_pipeline_cost_usd: float = 0.0
        total_pipeline_duration_seconds: float = 0.0
        currency: str = "USD"
        pricing_info_url: str = "Note: Prices are estimates based on known rates at the time of implementation (June 2025) and may vary. Check provider (OpenRouter/OpenAI) for current rates."
        cost_calculation_error: Optional[str] = None

    print("✅ Pydantic models defined.")
else:
    ImageAnalysisResult = RelevantNicheList = MarketingGoalSetStage2 = MarketingStrategyOutputStage2 = MarketingGoalSetFinal = VisualConceptDetails = StyleGuidance = StyleGuidanceList = ImageGenerationPrompt = CostDetail = PipelineCostSummary = None
    print("⚠️ Pydantic models cannot be defined because the library is not available.")

# @title Step 3: UI Input Widgets & Mode Selection (Enhanced with Creativity Level)

# --- Common Widgets ---
platform_selection = widgets.Dropdown(
    options=PLATFORM_DISPLAY_NAMES,
    value=PLATFORM_DISPLAY_NAMES[0],
    description='*Platform Target:',
    disabled=False,
    style={'description_width': 'initial'}
)

prompt_input = widgets.Textarea(
    value='',
    placeholder='Describe your product or what you want to generate...',
    description='Prompt:',
    layout={'height': '100px', 'width': '95%'}
)

image_upload = widgets.FileUpload(
    accept='image/*',
    multiple=False,
    description='Upload Image Ref:'
)
image_feedback_area = widgets.Output() # For upload confirmation/removal

image_instruction = widgets.Textarea(
    value='',
    placeholder='(Optional) e.g., "Use the burger as the main subject", "Enhance lighting".',
    description='Image Instruction:',
    layout={'height': '60px', 'width': '95%'}
)

render_text_toggle = widgets.Checkbox(
    value=False, description='Render Text?',
    tooltip='Check if text (e.g., from Task Content) should be rendered onto the final image.', indent=False
)
apply_branding_toggle = widgets.Checkbox(
    value=False, description='Apply Branding?',
    tooltip='Check if branding elements (logo, brand colors/fonts) should be applied.', indent=False
)

# --- NEW: Creativity Level Widget ---
creativity_level_input = widgets.SelectionSlider(
    options=[('1: Focused & Photorealistic', 1), ('2: Impressionistic & Stylized', 2), ('3: Abstract & Illustrative', 3)],
    value=2, # Default to Medium
    description='Creativity Level:',
    disabled=False,
    continuous_update=False,
    orientation='horizontal',
    readout=True,
    style={'description_width': 'initial'}
)

# --- Task-Specific & Custom Mode Widgets ---
task_selection = widgets.Dropdown(
    options=TASK_TYPES, value=TASK_TYPES[0], description='*Task Type:',
    disabled=False, style={'description_width': 'initial'}
)
branding_elements_input = widgets.Textarea(
    value='', placeholder='(Optional) e.g., "Logo in top-left, use #FFC107 for accents", "Font: Montserrat".',
    description='Branding:', layout={'height': '80px', 'width': '95%'}
)
task_description_input = widgets.Textarea(
    value='', placeholder='(Optional) e.g., "Promo: 2-for-1 Coffee!", "Menu: Signature Pasta", "Story: Our 5th Anniversary".',
    description='Task Content:', layout={'height': '80px', 'width': '95%'}
)
marketing_audience = widgets.Text(value='', placeholder='(Optional) e.g., Young professionals, families', description='Target Audience:')
marketing_objective = widgets.Text(value='', placeholder='(Optional) e.g., Increase engagement, drive sales', description='Objective:')
marketing_voice = widgets.Text(value='', placeholder='(Optional) e.g., Playful, sophisticated, casual', description='Voice:')
marketing_niche = widgets.Text(value='', placeholder='(Optional) e.g., Vegan cafe, fine dining', description='Niche:')

marketing_goals_box = widgets.VBox([
    widgets.HTML("<b>Optional Marketing Goals:</b>"),
    marketing_audience, marketing_objective, marketing_voice, marketing_niche
])

# --- Image Upload Handling ---
def clear_image_upload_ui(b=None):
    image_upload.value.clear() # Clear the FileUpload widget's internal value
    image_upload._counter +=1 # Force re-render if needed by ipywidgets version
    with image_feedback_area:
        clear_output(wait=True)
        print("No image selected.")
remove_image_button = widgets.Button(description="Remove Image", button_style='warning')
remove_image_button.on_click(clear_image_upload_ui)

def on_image_upload_change(change):
    with image_feedback_area:
        clear_output(wait=True)
        if change['new']:
            filename = list(change['new'].keys())[0]
            display(widgets.HTML(f"<b>Uploaded:</b> {filename}"), remove_image_button)
        else:
            print("No image selected.")
image_upload.observe(on_image_upload_change, names='value')
# Initialize feedback area
with image_feedback_area: print("No image selected.")


# --- Mode Selection and UI Update Logic ---
mode_selection = widgets.RadioButtons(
    options=['Easy Mode', 'Custom Mode', 'Task-Specific Mode'],
    description='Select Mode:', disabled=False
)

def reset_all_inputs_for_mode(mode_name_str):
    platform_selection.value = PLATFORM_DISPLAY_NAMES[0]
    prompt_input.value = ''
    clear_image_upload_ui()
    image_instruction.value = ''
    render_text_toggle.value = False
    apply_branding_toggle.value = False
    creativity_level_input.value = 2 # Reset creativity to default

    if mode_name_str in ['Custom Mode', 'Task-Specific Mode']:
        branding_elements_input.value = ''
        task_description_input.value = ''
        marketing_audience.value = ''
        marketing_objective.value = ''
        marketing_voice.value = ''
        marketing_niche.value = ''
    if mode_name_str == 'Task-Specific Mode':
        task_selection.value = TASK_TYPES[0]
    print(f"{mode_name_str} inputs reset.")

reset_button_easy = widgets.Button(description="Reset All Inputs", button_style='info', layout={'margin': '10px 0 0 0'})
reset_button_custom = widgets.Button(description="Reset All Inputs", button_style='info', layout={'margin': '10px 0 0 0'})
reset_button_task_specific = widgets.Button(description="Reset All Inputs", button_style='info', layout={'margin': '10px 0 0 0'})

reset_button_easy.on_click(lambda b: reset_all_inputs_for_mode('Easy Mode'))
reset_button_custom.on_click(lambda b: reset_all_inputs_for_mode('Custom Mode'))
reset_button_task_specific.on_click(lambda b: reset_all_inputs_for_mode('Task-Specific Mode'))

easy_mode_widgets = widgets.VBox([
    widgets.HTML("<h3>Easy Mode:</h3><p>Quick generation with minimal input. Focus on the prompt and/or image.</p>"),
    platform_selection, creativity_level_input, prompt_input,
    image_upload, image_feedback_area, image_instruction,
    render_text_toggle, apply_branding_toggle, reset_button_easy
])
custom_mode_widgets = widgets.VBox([
    widgets.HTML("<h3>Custom Mode:</h3><p>Provide detailed inputs for a highly tailored result.</p>"),
    platform_selection, creativity_level_input, prompt_input,
    image_upload, image_feedback_area, image_instruction,
    render_text_toggle, apply_branding_toggle,
    branding_elements_input, task_description_input, marketing_goals_box, reset_button_custom
])
task_specific_mode_widgets = widgets.VBox([
    widgets.HTML("<h3>Task-Specific Mode:</h3><p>Select an F&B task for guided input fields.</p>"),
    platform_selection, task_selection, creativity_level_input, prompt_input,
    image_upload, image_feedback_area, image_instruction,
    render_text_toggle, apply_branding_toggle,
    branding_elements_input, task_description_input, marketing_goals_box, reset_button_task_specific
])

input_container = widgets.VBox([])
main_output_area = widgets.Output() # For pipeline logs and results

def on_mode_change(change):
    with main_output_area: # Clear previous main output when mode changes to avoid confusion
        clear_output(wait=True)
        print("Mode changed. Configure inputs below and run the pipeline.")
    if change['new'] == 'Easy Mode': input_container.children = [easy_mode_widgets]
    elif change['new'] == 'Custom Mode': input_container.children = [custom_mode_widgets]
    elif change['new'] == 'Task-Specific Mode': input_container.children = [task_specific_mode_widgets]
    else: input_container.children = []
    # Preserve image upload feedback if an image was already selected
    with image_feedback_area:
        clear_output(wait=True) # Clear its own area first
        if image_upload.value:
            filename = list(image_upload.value.keys())[0]
            display(widgets.HTML(f"<b>Uploaded:</b> {filename}"), remove_image_button)
        else:
            print("No image selected.")

mode_selection.observe(on_mode_change, names='value')
on_mode_change({'new': mode_selection.value}) # Initialize display

print("Input widgets configured. Select a mode above to see relevant inputs.")
display(mode_selection, input_container) # Display UI

# @title Step 4: Helper Functions (Consolidated)

def extract_json_from_llm_response(raw_text: str) -> Optional[str]:
    """
    Extracts a JSON string from an LLM's raw text response.
    Handles markdown code blocks and attempts to parse direct JSON,
    including cases with "Extra data" errors.
    """
    if not isinstance(raw_text, str): # Ensure raw_text is a string
        return None

    # 1. Try to find JSON within ```json ... ```
    match_md_json = re.search(r"```json\s*([\s\S]+?)\s*```", raw_text, re.IGNORECASE)
    if match_md_json:
        json_str = match_md_json.group(1).strip()
        try:
            json.loads(json_str) # Validate
            return json_str
        except json.JSONDecodeError:
            pass # Continue to other methods if this fails

    # 2. Try to find JSON within ``` ... ``` (generic code block)
    match_md_generic = re.search(r"```\s*([\s\S]+?)\s*```", raw_text, re.IGNORECASE)
    if match_md_generic:
        potential_json = match_md_generic.group(1).strip()
        if (potential_json.startswith('{') and potential_json.endswith('}')) or \
           (potential_json.startswith('[') and potential_json.endswith(']')):
            try:
                json.loads(potential_json) # Validate
                return potential_json
            except json.JSONDecodeError:
                pass # Continue

    # 3. Try to parse the stripped raw_text directly
    stripped_text = raw_text.strip()
    if not stripped_text:
        return None

    try:
        json.loads(stripped_text) # Try to parse the whole stripped text
        return stripped_text # If successful, the whole thing is JSON
    except json.JSONDecodeError as e:
        # If "Extra data" error, it means a valid JSON object was parsed,
        # but there was trailing data. e.pos is the index of the start of extra data.
        if "Extra data" in str(e) and e.pos > 0:
            potential_json_substring = stripped_text[:e.pos]
            try:
                json.loads(potential_json_substring) # Re-validate the substring
                return potential_json_substring.strip()
            except json.JSONDecodeError:
                 # This can happen if the initial part wasn't actually complete JSON
                 # or if the LLM output is very malformed, e.g. "Here is the JSON: {incomplete..."
                 pass # Fall through to other methods
        # If it's not "Extra data" or e.pos is 0, it means the beginning itself is not JSON.
        # Example: "Sure, here is the JSON: {...}" - e.pos would be 0 if it fails immediately.
        # In this case, we try to find the first '{' or '['.

    # 4. Fallback: find the first '{' to the last '}' or first '[' to last ']'
    # This is a simpler heuristic if the above fail.
    first_brace = stripped_text.find('{')
    last_brace = stripped_text.rfind('}')
    first_bracket = stripped_text.find('[')
    last_bracket = stripped_text.rfind(']')

    json_candidate = None

    if first_brace != -1 and last_brace != -1 and first_brace < last_brace:
        potential_obj_str = stripped_text[first_brace : last_brace + 1]
        try:
            json.loads(potential_obj_str)
            json_candidate = potential_obj_str
        except json.JSONDecodeError:
            pass

    if first_bracket != -1 and last_bracket != -1 and first_bracket < last_bracket:
        potential_arr_str = stripped_text[first_bracket : last_bracket + 1]
        try:
            json.loads(potential_arr_str)
            # If an object was also found, prefer the one that's not inside the other,
            # or prefer object if ambiguity. For simplicity, if both are valid and standalone,
            # this might need more sophisticated logic if LLM mixes them.
            # Here, if an object was found, we'll stick with it unless the array is clearly not part of it.
            if json_candidate:
                # If array is outside or wraps the object, consider it
                if not (first_bracket > first_brace and last_bracket < last_brace):
                     json_candidate = potential_arr_str # Or decide based on which is "more complete"
            else:
                json_candidate = potential_arr_str
        except json.JSONDecodeError:
            pass

    return json_candidate

def get_pools_for_task(task_type_str: Optional[str]) -> Dict[str, List[str]]:
    """Returns the appropriate marketing goal option pools based on the task type string."""
    if not task_type_str: return TASK_GROUP_POOLS["default"]
    if task_type_str.startswith('1.') or task_type_str.startswith('4.'): return TASK_GROUP_POOLS["product_focus"]
    if task_type_str.startswith('2.'): return TASK_GROUP_POOLS["promotion"]
    if task_type_str.startswith('3.') or task_type_str.startswith('5.') or task_type_str.startswith('7.') or task_type_str.startswith('8.'): return TASK_GROUP_POOLS["brand_atmosphere"]
    if task_type_str.startswith('6.'): return TASK_GROUP_POOLS["informative"]
    return TASK_GROUP_POOLS["default"]

def simulate_image_evaluation_fallback(user_has_provided_instruction: bool) -> Dict[str, Any]:
    """Provides a simulated structured analysis if Pydantic model or LLM fails."""
    if not ImageAnalysisResult:
         return {"error": "Pydantic model ImageAnalysisResult not defined", "main_subject": "Simulated Subject (No Pydantic)"}
    data = {
        "main_subject": "Simulated Main Subject (e.g., Cupcake)",
        "secondary_elements": None, "setting_environment": None,
        "style_mood": None, "extracted_text": None,
    }
    if user_has_provided_instruction:
        data["secondary_elements"] = ["Simulated secondary item 1", "Simulated frosting detail"]
        data["setting_environment"] = "Simulated clean background"
        data["style_mood"] = "Simulated bright and cheerful"
    try:
      return ImageAnalysisResult(**data).model_dump()
    except Exception as e:
        print(f"Error creating fallback Pydantic ImageAnalysisResult object: {e}")
        return {"error": f"Fallback creation failed: {e}", "main_subject": "Error"}


def simulate_marketing_strategy_fallback_staged(user_goals: Optional[Dict], identified_niches: List[str], task_type: Optional[str], num_strategies: int = 3) -> List[Dict[str, Any]]:
    """Provides N simulated marketing strategies for the staged approach."""
    if not MarketingGoalSetFinal:
        return [{"error": "Pydantic model MarketingGoalSetFinal not defined"}] * num_strategies

    task_pools = get_pools_for_task(task_type)
    audience_pool = task_pools.get("audience", TASK_GROUP_POOLS["default"]["audience"])
    objective_pool = task_pools.get("objective", TASK_GROUP_POOLS["default"]["objective"])
    voice_pool = task_pools.get("voice", TASK_GROUP_POOLS["default"]["voice"])
    strategies = []
    used_combinations = set()
    if not isinstance(identified_niches, list) or not identified_niches:
        identified_niches = [random.choice(TASK_GROUP_POOLS["default"]["niche"])]

    user_provided_niche = (user_goals or {}).get('niche')
    user_provided_audience = (user_goals or {}).get('target_audience')
    user_provided_objective = (user_goals or {}).get('objective')
    user_provided_voice = (user_goals or {}).get('voice')

    for i in range(num_strategies):
        current_niche = user_provided_niche if user_provided_niche else identified_niches[i % len(identified_niches)]
        sim_audience = user_provided_audience or random.choice(audience_pool)
        sim_objective = user_provided_objective or random.choice(objective_pool)
        sim_voice = user_provided_voice or random.choice(voice_pool)
        current_goals = {
            "target_audience": sim_audience, "target_niche": current_niche,
            "target_objective": sim_objective, "target_voice": sim_voice
        }
        if user_goals and all((user_goals or {}).get(k) for k in ['target_audience', 'objective', 'voice', 'niche']) and i > 0:
            current_goals["target_objective"] += f" (SimVar {i})"
        combination_tuple = tuple(current_goals.values())
        attempts = 0
        while combination_tuple in used_combinations and attempts < 10:
            current_goals["target_objective"] = random.choice(objective_pool) + f" (AltSim {attempts})"
            combination_tuple = tuple(current_goals.values())
            attempts += 1
        if combination_tuple not in used_combinations:
            try:
                strategies.append(MarketingGoalSetFinal(**current_goals).model_dump())
                used_combinations.add(combination_tuple)
            except Exception as e:
                strategies.append({"error": f"Fallback strategy {i+1} creation failed: {e}", "target_niche": current_niche})
        elif len(strategies) < num_strategies:
             strategies.append({"error": "Could not generate unique fallback strategy", "target_niche": current_niche, **current_goals})
    while len(strategies) < num_strategies:
        if strategies: strategies.append(strategies[0]) # Duplicate first if any exist
        else: strategies.append({"error": "Fallback failed completely", "target_niche": identified_niches[0] if identified_niches else "DefaultNiche"})
    return strategies[:num_strategies] # Ensure exact number

def map_to_supported_aspect_ratio_for_prompt(aspect_ratio: str) -> str:
    """Maps input aspect ratio to a supported aspect ratio string (1:1, 2:3, 3:2) for use in the image generation prompt."""
    # This mapping is for the textual prompt, not necessarily the API size parameter
    if aspect_ratio == "1:1": return "1:1"
    elif aspect_ratio in ["9:16", "3:4", "2:3"]: return "2:3" # Vertical
    elif aspect_ratio == "16:9" or aspect_ratio == "1.91:1": return "3:2" # Horizontal
    else:
        print(f"Warning: Unsupported aspect ratio '{aspect_ratio}' for prompt text. Defaulting to '1:1'.")
        return "1:1"

def map_aspect_ratio_to_size_for_api(aspect_ratio: str) -> Optional[str]:
    """Maps aspect ratio string to size parameter supported(1024x1024, 1792x1024, 1024x1792)."""
    if aspect_ratio == "1:1": return "1024x1024"
    elif aspect_ratio in ["9:16", "3:4", "2:3"]: # Vertical
        print(f"Warning: Mapping aspect ratio '{aspect_ratio}' to supported vertical size '1024x1536' (2:3).")
        return "1024x1536"
    elif aspect_ratio in ["16:9", "1.91:1"]: # Horizontal
        print(f"Warning: Mapping aspect ratio '16:9' to supported horizontal size '1536x1024' (3:2).")
        return "1536x1024"
    else:
        print(f"Warning: Unsupported aspect ratio '{aspect_ratio}'. Defaulting to '1024x1024'.")
        return "1024x1024"

print("✅ Helper functions defined.")

# @title Step 5: Prompt Engineering Functions (get_system_prompt, get_user_prompt_parts - Consolidated)

# This combined get_system_prompt handles contexts for Style Guider and Creative Expert
def get_system_prompt(
    creativity_level: int,
    task_type: str,
    use_instructor_parsing: bool, # True if instructor is used for this call
    has_reference: bool, # For Creative Expert context
    has_instruction: bool, # For Creative Expert context
    render_text_flag: bool, # For Creative Expert context
    apply_branding_flag: bool, # For Creative Expert context
    platform_name: Optional[str] = "N/A", # For Creative Expert context
    # Agent-specific flags
    is_style_guider_prompt: bool = False,
    num_strategies_for_style_guider: Optional[int] = None, # For Style Guider context
    # Model specific flags
    target_model_family: Optional[str] = "openai" # e.g., "openai", "gemini", "anthropic", "qwen"
) -> str:
    """Returns the system prompt based on the agent type, creativity level, task type, and other context."""

    # --- Style Guider System Prompt ---
    if is_style_guider_prompt:
        creativity_desc_sg = {1: "focused and conventional", 2: "impressionistic and stylized", 3: "abstract and illustrative"}
        base_persona_sg = f"""
You are an expert Art Director and Style Consultant specializing in F&B social media visuals.
Your task is to generate {num_strategies_for_style_guider or 'N'} distinct and varied sets of high-level style guidance, each uniquely tailored to its corresponding marketing strategy and the task type: '{task_type}'.
The creativity level is {creativity_level} ({creativity_desc_sg.get(creativity_level, 'balanced')}). Styles must strictly adhere to the defined artistic boundaries for this level, ensuring a clear progression from photorealistic (Level 1) to impressionistic/stylized (Level 2) to abstract/illustrative (Level 3).
Each style guidance set must include a specific artistic constraint or reference to guide the Creative Expert, a detailed 2-3 sentence description, and an explanation of marketing impact for shareability, engagement, or brand recall.
"""
        style_diversification_sg = """
Ensure each style set is significantly distinct in aesthetic framework (e.g., composition, color palette, texture, inspiration source) to avoid repetitive or clichéd interpretations of user prompt keywords (e.g., avoid defaulting to neon for 'futuristic,' wood for 'rustic,' or warm tones for 'cozy').
For each prompt keyword, draw from diverse aesthetic lenses (e.g., cultural motifs like Japanese wabi-sabi or Moroccan patterns, historical periods like Art Deco or Bauhaus, environmental themes like desert minimalism or oceanic fluidity) to create varied, contextually relevant styles that align with the marketing strategy and task type.
Styles must be appropriate for the F&B domain and social media marketing, balancing creativity with marketability.
"""
        creativity_instruction_sg = ""
        if creativity_level == 1:
            creativity_instruction_sg = """
**Style Guidance (Level 1 - Focused & Photorealistic):**
Propose clear, professional, photorealistic, or minimally stylized styles (e.g., 'studio product photography', 'clean minimalism', 'bright macro shot').
Avoid any artistic or experimental styles.
Include a constraint, e.g., 'use symmetrical composition for maximum clarity' or 'employ even, natural lighting'.
Styles must prioritize product or subject clarity for broad audience appeal.
"""
        elif creativity_level == 3:
            creativity_instruction_sg = """
**Style Guidance (Level 3 - Abstract & Illustrative):**
Propose highly imaginative, abstract, or illustrative styles (e.g., 'surrealist food illustration', 'cubist composition', 'graphic novel art inspired by Lichtenstein').
Include a bold constraint, e.g., 'depict the subject as an abstract entity' or 'use a fully illustrative rendering with no photorealistic elements'.
Styles must be visually striking and shareable, while supporting the marketing strategy’s voice and objective.
"""
        else: # Level 2 (Default)
            creativity_instruction_sg = """
**Style Guidance (Level 2 - Impressionistic & Stylized):**
Propose creative, stylized styles with impressionistic or cinematic qualities (e.g., 'cinematic food noir with dramatic lighting', 'bold flat design with textured patterns', 'impressionist brushstroke with vibrant food tones').
Include a stylization constraint, e.g., 'use cinematic lighting with soft shadows', 'incorporate textured patterns for depth', or 'apply impressionistic color blending for vibrancy'.
Styles must be visually appealing, marketable, and distinct from photorealism, while explicitly avoiding fully illustrative, cartoonish, or abstract renderings (e.g., no anthropomorphic elements or surreal compositions unless specified).
"""
        task_type_awareness_sg = f"**Task Type Context:** The task is '{task_type}'. Ensure styles are appropriate for this task’s visual and marketing requirements."
        image_ref_context_sg = "**Image Reference Context:** If a reference image is provided (details in user prompt), ensure styles complement or transform it appropriately, maintaining focus on style diversity across strategies."
        marketing_impact_sg = "**Marketing Impact:** For each style, include a 'marketing_impact' field explaining how it supports social media marketing goals (e.g., 'vibrant colors drive engagement on Instagram', 'authentic style fosters trust on Xiaohongshu')."

        output_format_sg = ""
        if use_instructor_parsing and STYLE_GUIDER_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS:
            output_format_sg = "Output a list of JSON objects, each conforming to the `StyleGuidance` Pydantic model (fields: `style_keywords`, `style_description`, `marketing_impact`, `source_strategy_index`). Ensure `style_description` is 2-3 sentences, specifying artistic references or constraints. Styles must be distinct for each strategy."
        else: # Manual JSON parsing or if model is problematic with instructor's tool mode
            output_format_sg = """
VERY IMPORTANT: Format your entire response *only* as a valid JSON object with a single key 'style_guidance_sets'.
This key should contain a list of objects, each with 'style_keywords' (list of 3-5 strings), 'style_description' (2-3 sentence string), 'marketing_impact' (string), and 'source_strategy_index' (integer).
Do not include any other text or formatting outside this JSON structure.
"""
        prompt_parts_sg = [
            base_persona_sg, style_diversification_sg, creativity_instruction_sg,
            task_type_awareness_sg, image_ref_context_sg, marketing_impact_sg, output_format_sg
        ]
        if target_model_family == "qwen": prompt_parts_sg.append("   /no_thinking   ") # Qwen specific
        return "\n".join(prompt_parts_sg)

    # --- Creative Expert System Prompt ---
    base_persona_ce = """
You are an expert Creative Director and Digital Marketing Strategist specializing in F&B social media visuals.
Your primary objective is to generate a highly detailed, exceptionally creative, and effective *structured visual concept* based on the provided marketing strategy, style guidance, and context.
This structured concept will later be used to generate a prompt for a text-to-image generation model.
Your goal is to provide concepts that are significantly value-added, pushing beyond generic interpretations and getting the user much closer to their ideal image on the first attempt.
"""
    input_refinement_ce = """
**Input Refinement:** Critically review ALL user inputs (Original User Prompt Hint, Specific Task Content/Description, Branding Guidelines, Image Instruction, and the provided Style Guidance). If any input is brief, vague, contains grammatical errors, or seems misaligned, you MUST interpret the user's likely intent, refine it, expand upon it creatively, and clearly explain your refined interpretation within the relevant structured output fields. Ensure the final concept is coherent and aligns with the marketing strategy and task type.
"""
    core_task_ce = """
**Core Task:** Embody maximum creativity and imagination, coupled with a deep understanding of visual design principles (color, composition, lighting, typography), F&B marketing trends, and image generation capabilities. Generate diverse, high-impact visual concepts tailored to the specific task type and marketing goals.
**You will be provided with specific `Style Guidance` (keywords, description, marketing impact) for this concept. Your `visual_style` field MUST be a detailed and rich elaboration of this provided style, adhering to its artistic boundaries and constraint.** Use it as your primary stylistic foundation and creatively build upon it, ensuring all other visual elements (composition, lighting, color, main_subject description) harmonize with and bring this specific style to life in a way that is relevant to the task type and marketing strategy.
**Style and Task Synthesis:** The `Style Guidance` dictates the primary artistic treatment. You must creatively adapt the specific visual elements mentioned in the `Task Type Adaptation` guidance to fit *within* this overarching artistic style. For example, if the style is 'surrealist illustration' and the task is 'Product Photography' (which typically calls for clear product focus), interpret this to mean clear, dramatic, or stylized elements that enhance the surrealist illustration while still ensuring the product is recognizable and appealing, as guided by the task's Creativity Guardrail. The goal is a harmonious blend where the artistic style is paramount but is applied to meet the functional needs of the task type.
Fill in all the fields of the requested Pydantic JSON output format (`ImageGenerationPrompt` containing `VisualConceptDetails`). Be extremely specific, descriptive, and justify design choices implicitly through the descriptions. The `main_subject` field should describe all key subjects and their interaction clearly (unless instructed otherwise for default edits).
"""
    # Task Type Guidance Map (from downstream script, unchanged)
    task_type_guidance_details_map = {
        "Product Photography": {
            "base_description": "- 'Product Photography': Create visuals with exceptional clarity to showcase the product’s details, textures, and qualities for menu spotlight or sales.\n- **Visuals**: Use clean, complementary backgrounds (e.g., marble, wood) and professional, appetizing lighting (e.g., soft natural light, subtle highlights). Aim for visuals where the product is the clear hero, its key features are sharply defined, and it has high visual appeal, all rendered through the lens of the chosen artistic Style Guidance. Consider macro or medium shots.\n- **Creativity Guardrail**: Even at Level 3, abstract styles must clearly highlight the product’s form and appeal (e.g., surreal lighting but recognizable product).",
            "platform_optimization_detail": "- **Platform Optimization**: For Instagram, use square or vertical compositions with vibrant product focus; for Xiaohongshu, emphasize high-quality textures for lifestyle appeal.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, place promotional text (e.g., product name, price) in a clear, bold sans-serif font, perhaps in the bottom-right, ensuring it doesn’t obscure the product.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding elements (e.g., logo) subtly, perhaps in a top corner with ~50% opacity, ensuring it complements the overall aesthetic."
        },
        "Promotional Graphics & Announcements": { # Matched key from TASK_TYPES
            "base_description": "- 'Promotional Graphics': Design for immediate visual impact and attention-grabbing power to drive engagement.\n- **Visuals**: Use dynamic compositions (e.g., diagonal lines, bold focal points) and impactful colors (vibrant reds/yellows for sales, deep blues/golds for luxury, festive greens/reds for holidays). Styles can include graphic designs, illustrative elements, or stylized photo-manipulation, as guided by the Style Guidance.\n- **Creativity Guardrail**: At Level 3, abstract styles must retain clear promotional messaging (e.g., illustrative product with bold text).",
            "platform_optimization_detail": "- **Platform Optimization**: For Instagram Stories, use vertical formats with bold, centered text; for Facebook, ensure square compositions for shareability; for Xiaohongshu, add informative text overlays in blog-post style.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, use large, bold sans-serif headlines (e.g., top-center) and smaller serif or sans-serif captions (e.g., bottom-left) for hierarchy.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo bottom-right, brand colors in accents) to reinforce identity without cluttering the promotional message."
        },
        "Store Atmosphere & Decor": { # Matched key
            "base_description": "- 'Store Atmosphere': Focus on immersive environmental storytelling to capture the unique mood and ambiance of the F&B space.\n- **Visuals**: Include 1-2 contextual elements (e.g., customers reflecting target audience, staff in natural interactions) positioned in midground to emphasize decor. Use mood-driven lighting (e.g., warm for cozy, cool for chic), rendered through the chosen artistic Style Guidance.\n- **Creativity Guardrail**: At Level 3, abstract styles must convey the venue’s ambiance (e.g., surreal decor but recognizable setting).",
            "platform_optimization_detail": "- **Platform Optimization**: For Pinterest, use vertical shots showcasing decor details; for Instagram Stories, capture dynamic ambiance for Reels.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, place taglines in an elegant serif font, perhaps bottom-center, as a subtle overlay.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo etched on a surface) subtly within the scene."
        },
         "Menu Spotlights": {
            "base_description": "- 'Menu Spotlights': Favor close-up or medium shots to highlight a specific menu item. Backgrounds should enhance brand identity or campaign themes (e.g., festive settings, complementary textures).\n- **Visuals**: Use appetizing lighting (e.g., warm highlights, soft shadows) and styles from photorealistic to lightly stylized, all rendered through the lens of the chosen artistic Style Guidance, ensuring the dish is the hero.\n- **Creativity Guardrail**: Even at Level 3, abstract or highly stylized approaches must ensure the spotlighted menu item remains the clear focal point and is appetizingly represented.",
            "platform_optimization_detail": "- **Platform Optimization**: For Instagram, use square or vertical compositions with a vibrant dish focus; for Xiaohongshu, create contextual lifestyle shots that make the dish look appealing and aspirational.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, suggest bold sans-serif promotional text (e.g., dish name, key ingredient, or price) perhaps positioned bottom-center or strategically to enhance the dish without obscuring it.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo in a corner or subtly on tableware/napkin) ensuring it complements the menu item spotlight."
        },
        "Cultural & Community Content": { # Matched key
            "base_description": "- 'Cultural Content': Favor compositions with symbolic elements (e.g., lanterns for Lunar New Year, floral motifs for spring festivals, traditional patterns) and culturally inspired color palettes (e.g., reds/golds for prosperity, pastels for spring).\n- **Visuals**: Use styles from photorealistic to illustrative with mood-driven lighting (e.g., festive glow, warm ambient light, dramatic cultural lighting), all interpreted via the Style Guidance to evoke the cultural theme respectfully and beautifully.\n- **Creativity Guardrail**: At Level 3, abstract or illustrative styles must clearly convey the cultural theme and its intended emotional resonance (e.g., festive, respectful, celebratory, nostalgic).",
            "platform_optimization_detail": "- **Platform Optimization**: For Xiaohongshu, create lifestyle-oriented visuals with detailed captions explaining cultural significance or personal stories; for Pinterest, design vertical, symbolic visuals that are aesthetically pleasing and shareable, often incorporating text overlays.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, suggest elegant serif or culturally appropriate script taglines or relevant phrases, perhaps positioned centrally or at the bottom, harmonizing with the visual elements.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo subtly incorporated into cultural motifs or as a respectful watermark) in a way that honors and complements the cultural context."
        },
        "Recipes & Food Tips": { # Matched key
            "base_description": "- 'Recipes & Tips': Prioritize clarity, visual instruction, and appetite appeal for educational content.\n- **Visuals**: Use consistent top-down or 45-degree angles for steps or final dishes, with clean, bright styles and unique color palettes (e.g., pastel greens for fresh recipes), all interpreted via the Style Guidance. Ensure visuals are suitable for series content with cohesive framing.\n- **Creativity Guardrail**: At Level 3, abstract styles must maintain instructional clarity (e.g., illustrative ingredients but clear steps).",
            "platform_optimization_detail": "- **Platform Optimization**: For Pinterest, create vertical infographic-style pins with text overlays; for Xiaohongshu, use detailed captions for instructional appeal.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, use concise, bold sans-serif text for step titles (e.g., top-left) and smaller, clear sans-serif for instructions (e.g., bottom).",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo in a corner) to tie to brand identity without distracting from the instructional content."
        },
        "Brand Story & Milestones": { # Matched key
            "base_description": "- 'Brand Story/Milestones': Create evocative, narrative, or celebratory visuals to resonate emotionally.\n- **Visuals**: Use compositions reflecting the story (e.g., nostalgic sepia for heritage, futuristic neons for innovation), with symbolic elements. Styles can range from heartfelt interpretations of photorealism to artistic expressions, guided by the Style Guidance. Ensure brand’s primary color appears in approximately 30% of the composition if appropriate for the style.\n- **Creativity Guardrail**: At Level 3, abstract styles must reflect brand voice (e.g., surreal milestone but tied to brand colors).",
            "platform_optimization_detail": "- **Platform Optimization**: For Facebook, use square, shareable visuals; for Instagram, create cinematic Reels with narrative flow.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, place taglines in an elegant serif font, perhaps center, as a focal point or integrated into the narrative.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo subtly watermarked) to reinforce identity within the storytelling."
        },
        "Behind the Scenes Imagery": { # Matched key
            "base_description": "- 'Behind the Scenes': Convey authenticity, process, and human element with professional yet candid visuals.\n- **Visuals**: Use soft, natural lighting and documentary-like compositions (e.g., candid shots of chefs, tools in action), interpreted through the Style Guidance. Avoid grainy or low-quality effects to maintain polish.\n- **Creativity Guardrail**: At Level 3, abstract styles must retain authenticity (e.g., illustrative process but recognizable human element).",
            "platform_optimization_detail": "- **Platform Optimization**: For Instagram, use square candid shots; for Xiaohongshu, include lifestyle-oriented captions.",
            "text_guidance_detail": "- **Text Details**: If text is enabled, place captions in a playful or clean sans-serif font, perhaps bottom-right, in a way that feels authentic.",
            "branding_guidance_detail": "- **Branding Details**: If branding is enabled, integrate branding (e.g., logo on tools or apparel) subtly and naturally within the scene."
        }
    }
    task_key_ce = None
    if task_type and isinstance(task_type, str): task_key_ce = task_type.split('.', 1)[-1].strip() if '.' in task_type else task_type
    effective_task_type_for_prompt_ce = task_type if task_type and task_type != TASK_TYPES[0] else "General Creative Task"
    selected_task_details_ce = task_type_guidance_details_map.get(task_key_ce)
    task_guidance_parts_ce = []
    if selected_task_details_ce:
        task_guidance_parts_ce.append(selected_task_details_ce.get("base_description", ""))
        if selected_task_details_ce.get("platform_optimization_detail"): task_guidance_parts_ce.append(selected_task_details_ce["platform_optimization_detail"])
        if render_text_flag and selected_task_details_ce.get("text_guidance_detail"): task_guidance_parts_ce.append(selected_task_details_ce["text_guidance_detail"])
        if apply_branding_flag and selected_task_details_ce.get("branding_guidance_detail"): task_guidance_parts_ce.append(selected_task_details_ce["branding_guidance_detail"])
        specific_task_guidance_ce = "\n".join(filter(None, task_guidance_parts_ce))
    else:
        specific_task_guidance_ce = f"- Adapt the visual concept appropriately for the specified Task Type ('{effective_task_type_for_prompt_ce}'). If no specific task type guidance is available or the task is general, focus on the overall marketing strategy and Style Guidance to create a compelling visual."
    task_type_awareness_ce = f"**Task Type Adaptation (CRUCIAL):** The specified `Task Type` is '{effective_task_type_for_prompt_ce}'. Your visual concept, including style, composition, and mood, MUST be expertly tailored to the requirements and goals of this specific task. Guidance:\n{specific_task_guidance_ce}"

    text_branding_field_instruction_ce = "**Text & Branding (JSON Output Fields):**\n"
    if render_text_flag: text_branding_field_instruction_ce += "- For `promotional_text_visuals` field: If text is needed based on the task or `task_description`, populate this field in the JSON output. Describe the refined text content, its visual style (e.g., impactful headline, informative caption), specific font characteristics (e.g., bold geometric sans-serif, elegant serif, playful script), placement hierarchy (e.g., headline top-center, subtext bottom-left), and seamless integration with the visual (e.g., text wrap around subject, subtle overlay, integrated into scene elements).\n"
    else: text_branding_field_instruction_ce += "- The `promotional_text_visuals` field in the JSON output MUST be omitted (set to null) as text rendering is disabled.\n"
    if apply_branding_flag: text_branding_field_instruction_ce += "- For `branding_visuals` field: If `Branding Guidelines` are provided, analyze them and describe in this JSON field how these elements (logo placeholders, taglines, specific brand fonts/colors) should be integrated elegantly and effectively (e.g., logo subtly watermarked, tagline etched onto a surface, brand color used as accent lighting). Refine branding input if necessary for clarity. If no guidelines are provided, state 'No specific branding guidelines provided; visual style derived from strategy and task.' and ensure the overall concept inherently reflects the brand's likely aesthetic based on the strategy.\n"
    else: text_branding_field_instruction_ce += "- The `branding_visuals` field in the JSON output MUST be omitted (set to null) as branding application is disabled.\n"

    reasoning_ce = "**Creative Reasoning:** After defining the visual concept, provide a brief explanation in the `creative_reasoning` field, connecting the key visual choices (style, mood, composition, subject focus, color palette) back to the core marketing strategy (audience, niche, objective, voice), the specific `Task Type`, the provided `Style Guidance`, and any significant user inputs or refinements made, especially noting how the image reference was handled. **Justify why the chosen creative direction is effective and aligns with the overall marketing objectives from the strategy.**"

    adherence_ce = ""
    if use_instructor_parsing and CREATIVE_EXPERT_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS:
        adherence_ce = "Adhere strictly to the requested Pydantic JSON output format (`ImageGenerationPrompt` containing `VisualConceptDetails`). Note that `main_subject`, `promotional_text_visuals`, and `branding_visuals` are optional and should be omitted (set to null) if the specific scenario instructs it (e.g., default edit, user flags). Ensure all other required descriptions are detailed enough to guide image generation effectively."
    else: # Manual JSON parsing or if model is problematic with instructor's tool mode
        adherence_ce = """
VERY IMPORTANT: Format your entire response *only* as a valid JSON object conforming to the structure described below. Do not include any introductory text, explanations, or markdown formatting outside the JSON structure itself.
JSON Structure:
{
  "visual_concept": {
    "main_subject": "string | null", // Omit (set to null) if default edit scenario
    "composition_and_framing": "string",
    "background_environment": "string",
    "foreground_elements": "string | null",
    "lighting_and_mood": "string",
    "color_palette": "string",
    "visual_style": "string", // Must elaborate on provided Style Guidance
    "promotional_text_visuals": "string | null", // Omit (set to null) if render_text_flag is false
    "branding_visuals": "string | null", // Omit (set to null) if apply_branding_flag is false
    "texture_and_details": "string | null",
    "negative_elements": "string | null",
    "creative_reasoning": "string | null"
  },
  "source_strategy_index": "integer | null" // This will be added programmatically later
}
Ensure all descriptions are detailed enough to guide image generation effectively.
"""
    creativity_instruction_ce = ""
    if creativity_level == 1:
        creativity_instruction_ce = """
**Creativity Guidance (Level 1 - Focused & Photorealistic):**
Prioritize clear, professional, photorealistic, or minimally stylized visuals (e.g., polished studio photography, clean minimalism) that strictly adhere to inputs and marketing strategy.
Use standard compositions (e.g., centered product shots, symmetrical framing) and proven styles.
**Constraint**: Use symmetrical composition or even, natural lighting to maximize clarity and product focus.
Avoid unconventional or artistic concepts to ensure reliability and broad appeal.
**Marketing Guardrail**: Ensure the visual prioritizes product or subject clarity to drive direct engagement or sales, aligning with the overall marketing objectives from the strategy.
"""
    elif creativity_level == 3:
        creativity_instruction_ce = """
**Creativity Guidance (Level 3 - Abstract & Illustrative):**
Create exceptionally imaginative, memorable, and high-impact visual concepts using abstract or illustrative styles (e.g., surrealist food illustration, cubist composition, graphic novel art).
**Constraint**: Select one from the following to shape the concept:
- Use a strictly monochromatic or duotone color scheme based on the strategy’s mood.
- Frame the composition as a specific art form (e.g., graphic novel panel, baroque oil painting).
- Depict the product as an abstract entity or symbolic character (e.g., a drink as a mythical creature).
- Integrate a strong visual metaphor tied to the brand’s voice (e.g., coffee as glowing energy sparks).
- Place the subject in a surreal or fantastical setting (e.g., a café on a cloud).
Explain the constraint’s impact in `creative_reasoning`.
Aim for novel, shareable concepts, but ensure the task’s marketing goal (e.g., product visibility, brand recall) is maintained, aligning with the overall marketing objectives from the strategy.
**Marketing Guardrail**: Even abstract styles must support the overall marketing objectives from the strategy (e.g., highlight product in surreal settings for Product Photography if the objective is product focus).
"""
    else: # Level 2 (Default)
        creativity_instruction_ce = """
**Creativity Guidance (Level 2 - Impressionistic & Stylized):**
Generate visually appealing, marketable concepts with impressionistic or stylized flair (e.g., cinematic food noir with dramatic lighting, bold flat design with textured patterns, impressionist brushstroke with vibrant food tones).
Use cinematic lighting, dynamic angles, or textured stylization, avoiding generic photorealism.
**Constraint**: Incorporate impressionistic color blending, dynamic angles, or textured stylization to enhance visual interest.
**Guardrail**: Explicitly avoid fully illustrative, cartoonish, or abstract renderings (e.g., no anthropomorphic elements, graphic novel art, or surreal compositions) unless explicitly specified by the Style Guider’s keywords or description.
Ensure concepts are memorable while aligning with the marketing strategy and task type.
**Marketing Guardrail**: Styles must balance creativity with clarity to support engagement and brand recognition, aligning with the overall marketing objectives from the strategy.
"""
    image_ref_handling_ce = "**Handling Image Reference (CRITICAL):**\n"
    if has_reference:
        if has_instruction: image_ref_handling_ce += "- An image reference IS provided AND a specific user `instruction` IS given: Interpret the instruction and apply it when describing the concept (e.g., describe the style in `visual_style`, describe the subject in `main_subject`, describe a new background in `background_environment`). Populate all fields including `main_subject`.\n"
        else: image_ref_handling_ce += "- An image reference IS provided BUT NO specific user `instruction` is given: The **primary subject** of the visual concept MUST be the analyzed subject from the reference image. Your main creative task is to design the *context* around this subject (composition, background, lighting, style, etc.) aligned with the marketing strategy. **Crucially, in this specific scenario, you MUST OMIT the `main_subject` field entirely (set it to null or do not include it) in your JSON output.** Focus ONLY on describing the context fields.\n"
    else: image_ref_handling_ce += "- NO image reference is provided: Generate the entire visual concept based on the marketing strategy and other inputs, including the `main_subject`.\n"

    prompt_parts_ce = [
        base_persona_ce, input_refinement_ce, core_task_ce, task_type_awareness_ce,
        creativity_instruction_ce, image_ref_handling_ce, text_branding_field_instruction_ce,
        reasoning_ce, adherence_ce
    ]
    if target_model_family == "qwen": prompt_parts_ce.append("   /no_thinking   ") # Qwen specific
    return "\n".join(prompt_parts_ce)


def get_user_prompt_parts( # For Creative Expert
    platform_name: str,
    aspect_ratio_for_prompt: str, # The mapped aspect ratio for prompt text
    strategy: Dict[str, Any], # MarketingGoalSetFinal structure
    task_type: str,
    user_prompt_original: Optional[str],
    task_description: Optional[str],
    branding_elements: Optional[str],
    render_text_flag: bool,
    apply_branding_flag: bool,
    has_image_reference: bool,
    saved_image_filename: Optional[str], # Filename of user's uploaded image
    image_subject_from_analysis: Optional[str],
    image_instruction: Optional[str], # User's instruction for the reference image
    use_instructor_parsing: bool, # True if instructor is used for this call
    is_default_edit: bool, # True if (has_image_reference AND NOT image_instruction)
    style_guidance_item: Optional[StyleGuidance] # The StyleGuidance object for this strategy
) -> List[str]:
    """Constructs the user prompt parts for the Creative Expert agent."""
    user_prompt_parts = [
        f"Generate a structured visual concept for an image targeting the '{platform_name}' platform (intended visual aspect ratio for prompt: {aspect_ratio_for_prompt}).",
        f"The core marketing strategy for this image is:",
        f"- Target Audience: {strategy.get('target_audience', 'N/A')}",
        f"- Target Niche: {strategy.get('target_niche', 'N/A')}",
        f"- Target Objective: {strategy.get('target_objective', 'N/A')}",
        f"- Target Voice: {strategy.get('target_voice', 'N/A')}",
        f"\nConsider the overall task context:",
        f"- Task Type: {task_type} (Ensure the concept strongly reflects the requirements of this task type as detailed in the system prompt).",
    ]

    if style_guidance_item:
        user_prompt_parts.append("\n**Style Direction to Follow (Provided by Style Guider):**")
        user_prompt_parts.append(f"- Style Keywords: {', '.join(style_guidance_item.style_keywords if style_guidance_item.style_keywords else [])}")
        user_prompt_parts.append(f"- Style Description: {style_guidance_item.style_description if style_guidance_item.style_description else 'N/A'}")
        user_prompt_parts.append(f"- Marketing Impact of this Style: {style_guidance_item.marketing_impact if style_guidance_item.marketing_impact else 'N/A'}")
        user_prompt_parts.append("   Your `visual_style` field description in the JSON output MUST be a detailed and creative elaboration of these provided style elements, adhering to its artistic boundaries and constraints.")
    else:
        user_prompt_parts.append("\n**Style Direction to Follow:** No specific style guidance provided by Style Guider; invent a style based on the creativity level (from system prompt) and other inputs.")

    if user_prompt_original: user_prompt_parts.append(f"- Original User Prompt Hint: '{user_prompt_original}' (Interpret and refine this hint if it's brief or unclear, integrating its essence into the concept).")
    text_render_status = "(Text rendering enabled by user)" if render_text_flag else "(Text rendering DISABLED by user)"
    if task_description: user_prompt_parts.append(f"- Specific Task Content/Description: '{task_description}' {text_render_status} (Interpret/refine content. If rendering enabled, describe visualization in `promotional_text_visuals` field of JSON output, following task-specific text guidance from system prompt).")
    elif render_text_flag: user_prompt_parts.append(f"- Specific Task Content/Description: Not provided, but text rendering is enabled. If relevant to task type, describe visualization in `promotional_text_visuals` field of JSON output, following task-specific text guidance from system prompt.")
    else: user_prompt_parts.append("- Specific Task Content/Description: Not provided, and text rendering is disabled.")

    branding_apply_status = "(Branding application enabled by user)" if apply_branding_flag else "(Branding application DISABLED by user)"
    if branding_elements: user_prompt_parts.append(f"- Branding Guidelines: '{branding_elements}' {branding_apply_status} (Interpret/refine guidelines. If application enabled, describe visualization in `branding_visuals` field of JSON output, following task-specific branding guidance from system prompt).")
    elif apply_branding_flag: user_prompt_parts.append(f"- Branding Guidelines: Not Provided, but branding application is enabled. Derive branding style from strategy/task and describe visualization in `branding_visuals` field of JSON output, following task-specific branding guidance from system prompt.")
    else: user_prompt_parts.append("- Branding Guidelines: Not Provided, and branding application is disabled.")

    user_prompt_parts.append("\nImage Reference Context (as detailed in system prompt):")
    if has_image_reference:
        user_prompt_parts.append(f"- An image reference was provided (Filename: {saved_image_filename or 'N/A'}).")
        if image_subject_from_analysis: user_prompt_parts.append(f"- Analysis identified the main subject of reference as: '{image_subject_from_analysis}'.")
        if image_instruction: user_prompt_parts.append(f"- User Instruction for reference image: '{image_instruction}' (Interpret and refine this instruction. Apply it carefully when describing the visual concept fields for the *new* image, as per system prompt guidance for instructed edits).")
        else: user_prompt_parts.append(f"- No specific instruction provided for the reference image. **Default edit behavior applies: The primary subject MUST be the analyzed subject ('{image_subject_from_analysis or 'Unknown'}'). Your task is to design the surrounding context ONLY. DO NOT describe the main subject in the `main_subject` field of the output JSON (leave it null/omit it).**")
    else: user_prompt_parts.append("- No image reference was provided. Generate the full concept including the `main_subject`.")

    platform_guidance_map = {
        "Instagram Post (1:1 Square)": "Optimize for Instagram Feed: Aim for a polished, visually cohesive aesthetic. Consider compositions suitable for square or vertical feeds. Ensure text placement is easily readable.",
        "Instagram Story/Reel (9:16 Vertical)": "Optimize for Instagram Story/Reel: Focus on dynamic, attention-grabbing visuals for a vertical format. **Describe the visual as if it were a single, high-impact frame from a video Reel.** Incorporate a sense of motion or action in the `composition_and_framing` description (e.g., 'dynamic motion blur,' 'subject captured mid-action,' 'cinematic freeze-frame effect').",
        "Facebook Post (Mixed)": "Optimize for Facebook Feed: Design for broad appeal and shareability. Ensure clear branding and messaging for potential ad use.",
        "Pinterest Pin (2:3 Vertical)": "Optimize for Pinterest: Create visually striking, informative vertical images. **If text is enabled, the concept MUST include a prominent text overlay.** As per Pinterest best practices, the description in `promotional_text_visuals` should specify text that is **large, highly legible (e.g., bold sans-serif fonts), and contains primary keywords from the marketing strategy.**",
        "Xiaohongshu (Red Note) (3:4 Vertical)": "Optimize for Xiaohongshu: Focus on an **authentic, User-Generated Content (UGC) aesthetic**. The concept should resemble a high-quality photo from a peer, not a polished ad. When describing the `visual_style`, favor terms like 'natural lighting' or 'candid shot.' **The concept should feature real people** interacting with the product or in the scene. If text is enabled, the `promotional_text_visuals` description must detail a **catchy, keyword-rich title overlay to act as a strong hook.**",
    }
    platform_guidance_text = platform_guidance_map.get(platform_name, f"Adapt the concept for the target platform '{platform_name}'.") if platform_name and platform_name != PLATFORM_DISPLAY_NAMES[0] else "Adapt the concept for general social media use."
    user_prompt_parts.append(f"\n**Platform Optimization (General Reminder):** {platform_guidance_text} (Detailed task-specific platform optimization is in system prompt).")

    final_instruction = f"""
\nBased on ALL the above context (especially the core marketing strategy, the provided Style Direction, and the task-specific guidance from the system prompt) and your expertise (refining user inputs as needed), generate the `ImageGenerationPrompt` JSON object.
Ensure the nested `VisualConceptDetails` object is fully populated with rich, descriptive details suitable for guiding a text-to-image model.
"""
    if is_default_edit: final_instruction += "- **IMPORTANT REMINDER: Since this is a default edit scenario (reference image provided, no specific user instruction), OMIT the `main_subject` field (set to null) in your JSON response. Focus ONLY on describing the context fields.**\n"
    else: final_instruction += "- Describe the `main_subject` clearly (following image reference logic from system prompt if applicable). This field should encompass all key subjects in the scene and their interactions.\n"

    final_instruction += """
- Detail the `composition_and_framing`.
- Describe the `background_environment`.
- Mention any important `foreground_elements`.
- Specify the `lighting_and_mood`.
- Define the `color_palette`.
- Articulate the `visual_style`. This field MUST comprehensively describe the desired aesthetic, being a detailed and creative elaboration of the provided Style Guidance. If no style guidance was given, invent a style according to the creativity level set in the system prompt.
"""
    if render_text_flag: final_instruction += "- If text is being rendered, ensure your description for `promotional_text_visuals` is detailed and creative, following any task-specific text guidance provided in the system prompt.\n"
    else: final_instruction += "- Omit `promotional_text_visuals` field (set to null) in JSON as text rendering is disabled.\n"
    if apply_branding_flag: final_instruction += "- If branding is being applied, ensure your description for `branding_visuals` is detailed and creative, following any task-specific branding guidance provided in the system prompt. Handle the case where no branding guidelines were provided.\n"
    else: final_instruction += "- Omit `branding_visuals` field (set to null) in JSON as branding application is disabled.\n"
    final_instruction += """
- Add notes on `texture_and_details` if relevant.
- List any `negative_elements` to avoid.
- **Provide a brief `creative_reasoning` explaining how the main visual choices connect to the core marketing strategy (especially the `target_objective`), user inputs, task type, and style guidance.**
- **Generate a concise and descriptive `suggested_alt_text` for SEO and accessibility (descriptive text only, no hashtags or promotional language).**

Ensure the overall visual concept aligns strongly with the core marketing strategy, task type '{task_type}', and incorporates the image reference context as instructed in the system prompt.
The `source_strategy_index` field in the JSON will be added programmatically later.
"""
    if not use_instructor_parsing and CREATIVE_EXPERT_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS : # Adjusted condition
         final_instruction += "\nVERY IMPORTANT: Your entire response MUST be only the JSON object described in the system prompt (Creative Expert section), starting with `{` and ending with `}`. Do not include any other text or formatting."

    user_prompt_parts.append(final_instruction)
    return user_prompt_parts

# For Style Guider Agent
def get_style_guider_user_prompt(
    strategies: List[Dict[str, Any]], # List of MarketingGoalSetFinal
    task_type: str,
    image_subject_from_analysis: Optional[str],
    user_prompt_original: Optional[str],
    num_strategies: int,
    use_instructor_parsing: bool
) -> str:
    """Constructs the user prompt for the Style Guider agent."""
    prompt_parts = [
        f"Generate {num_strategies} distinct style guidance sets, one for each of the following {num_strategies} marketing strategies. The overall F&B task is: '{task_type}'.",
        "The marketing strategies are as follows:"
    ]
    for i, strategy in enumerate(strategies):
        prompt_parts.append(f"\nStrategy {i}:")
        prompt_parts.append(f"  - Audience: {strategy.get('target_audience', 'N/A')}")
        prompt_parts.append(f"  - Niche: {strategy.get('target_niche', 'N/A')}")
        prompt_parts.append(f"  - Objective: {strategy.get('target_objective', 'N/A')}")
        prompt_parts.append(f"  - Voice: {strategy.get('target_voice', 'N/A')}")

    if image_subject_from_analysis: prompt_parts.append(f"\nConsider that the primary visual subject (from image analysis, if an image was provided) is: '{image_subject_from_analysis}'. Styles should complement or creatively transform this if applicable.")
    if user_prompt_original: prompt_parts.append(f"\nUser's original prompt hint for overall context: '{user_prompt_original}'.")

    prompt_parts.append(f"\nFor each of the {num_strategies} strategies, provide a `style_keywords` list (3-5 keywords), a detailed `style_description` (2-3 sentences including artistic constraints/references), and a `marketing_impact` statement. Ensure styles are significantly distinct across strategies and adhere to the creativity level guidance provided in the system prompt. The `source_strategy_index` for each style guidance set should correspond to the strategy index (0 to {num_strategies-1}).")

    if not use_instructor_parsing and STYLE_GUIDER_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS: # Adjusted condition
        prompt_parts.append("\nVERY IMPORTANT: Your entire response MUST be only the JSON object described in the system prompt (Style Guider section), starting with `{\"style_guidance_sets\": [` and ending with `]}`. Do not include any other text or formatting.")
    return "\n".join(prompt_parts)

print("✅ Prompt engineering functions defined.")

# @title Step 6: Core Pipeline Processing Functions

def gather_all_inputs() -> Dict[str, Any]:
    """Gathers all inputs from the UI widgets and processes image data."""
    inputs_data = {}
    mode = mode_selection.value
    inputs_data['mode'] = mode
    inputs_data['platform_selection_value'] = platform_selection.value # Store the raw value
    inputs_data['prompt'] = prompt_input.value if prompt_input.value else None
    inputs_data['image_widget_has_value'] = bool(image_upload.value) # Check if widget has a file
    inputs_data['render_text'] = render_text_toggle.value
    inputs_data['apply_branding'] = apply_branding_toggle.value
    inputs_data['image_instruction'] = image_instruction.value if image_instruction.value else None
    inputs_data['creativity_level'] = creativity_level_input.value # Get creativity level

    inputs_data['image_details'] = None
    inputs_data['image_content_base64'] = None
    inputs_data['image_content_bytes'] = None

    if image_upload.value: # If a file is actually uploaded
        try:
            # Assuming single file upload, image_upload.value is a dict
            # {'filename.jpg': {'metadata': ..., 'content': ...}}
            first_file_key = list(image_upload.value.keys())[0]
            file_data = image_upload.value[first_file_key]
            metadata = file_data['metadata']
            image_bytes = file_data['content']

            inputs_data['image_details'] = {
                'filename': metadata.get('name'),
                'content_type': metadata.get('type'),
                'size_bytes': metadata.get('size')
            }
            inputs_data['image_content_base64'] = base64.b64encode(image_bytes).decode('utf-8')
            inputs_data['image_content_bytes'] = image_bytes
            print(f"  Image '{metadata.get('name')}' processed for inputs.")
        except Exception as e:
            print(f"  Error processing uploaded image for inputs: {e}")
            inputs_data['image_details'] = None # Ensure reset on error
            inputs_data['image_content_base64'] = None
            inputs_data['image_content_bytes'] = None
    else:
        print("  No image uploaded by user.")

    if mode in ['Custom Mode', 'Task-Specific Mode']:
        inputs_data['branding_elements'] = branding_elements_input.value if branding_elements_input.value else None
        inputs_data['task_description'] = task_description_input.value if task_description_input.value else None
        inputs_data['mkt_audience'] = marketing_audience.value if marketing_audience.value else None
        inputs_data['mkt_objective'] = marketing_objective.value if marketing_objective.value else None
        inputs_data['mkt_voice'] = marketing_voice.value if marketing_voice.value else None
        inputs_data['mkt_niche'] = marketing_niche.value if marketing_niche.value else None
    if mode == 'Task-Specific Mode':
        inputs_data['task_type_selection_value'] = task_selection.value # Store raw value

    return inputs_data

def validate_inputs(mode: str, inputs_data: Dict[str, Any]) -> Tuple[bool, str]:
    """Performs basic validation on the gathered inputs_data."""
    if not inputs_data.get('platform_selection_value') or inputs_data['platform_selection_value'] == PLATFORM_DISPLAY_NAMES[0]:
        return False, "Client-Side Validation Error: A target Social Media Platform must be selected."

    if mode in ['Easy Mode', 'Custom Mode']:
        if not inputs_data.get('prompt') and not inputs_data.get('image_details'):
            return False, f"Client-Side Validation Error: {mode} requires either a text prompt or an uploaded image."
    elif mode == 'Task-Specific Mode':
        if not inputs_data.get('task_type_selection_value') or inputs_data['task_type_selection_value'] == TASK_TYPES[0]:
             return False, "Client-Side Validation Error: Task-Specific Mode requires selecting a valid Task Type."

    if inputs_data.get('image_widget_has_value') and not inputs_data.get('image_details'):
         return False, "Client-Side Validation Error: Image was uploaded but details could not be extracted."
    return True, "Client-Side Validation: Inputs seem valid for the selected mode."

def generate_initial_pipeline_data(mode: str, inputs_data: Dict[str, Any]) -> Dict[str, Any]:
    """Creates the initial structured pipeline_data dictionary."""
    image_ref_data = None
    if inputs_data.get('image_details'):
        image_ref_data = {
            "filename": inputs_data['image_details'].get('filename'),
            "content_type": inputs_data['image_details'].get('content_type'),
            "size_bytes": inputs_data['image_details'].get('size_bytes'),
            "instruction": inputs_data.get('image_instruction', None),
            "image_content_base64": inputs_data.get('image_content_base64', None)
            # "saved_image_path" will be added later if image is saved
        }
    marketing_goals_data = {
        "target_audience": inputs_data.get('mkt_audience') if inputs_data.get('mkt_audience') else None,
        "objective": inputs_data.get('mkt_objective') if inputs_data.get('mkt_objective') else None,
        "voice": inputs_data.get('mkt_voice') if inputs_data.get('mkt_voice') else None,
        "niche": inputs_data.get('mkt_niche') if inputs_data.get('mkt_niche') else None
    }
    if all(v is None for v in marketing_goals_data.values()): marketing_goals_data = None

    # Resolve task_type and platform from their selection values
    selected_task_type = inputs_data.get('task_type_selection_value', TASK_TYPES[0])
    actual_task_type = selected_task_type if selected_task_type != TASK_TYPES[0] else None

    selected_platform_name = inputs_data.get('platform_selection_value', PLATFORM_DISPLAY_NAMES[0])
    platform_details = SOCIAL_MEDIA_PLATFORMS.get(selected_platform_name) if selected_platform_name != PLATFORM_DISPLAY_NAMES[0] else None


    pipeline_data = {
      "pipeline_settings": {
          "run_timestamp": datetime.datetime.now().strftime("%Y%m%d_%H%M%S_%f"),
          "creativity_level_selected": inputs_data.get('creativity_level', 2) # Default if not found
      },
      "request_details": {
        "mode": mode.lower().replace(" ", "_").replace("-", "_"),
        "task_type": actual_task_type if mode == 'Task-Specific Mode' else None,
        "target_platform": { # Store resolved platform info
            "name": selected_platform_name if platform_details else None,
            "resolution_details": platform_details
        } if platform_details else None
      },
      "user_inputs": {
        "prompt": inputs_data.get('prompt', None),
        "image_reference": image_ref_data, # This already contains base64
        "render_text": inputs_data.get('render_text', False),
        "apply_branding": inputs_data.get('apply_branding', False)
      },
      "processing_context": {
          "initial_json_valid": None, # Server-side validation status
          "image_analysis_result": None,
          "suggested_marketing_strategies": None,
          "style_guidance_sets": None,
          "generated_image_prompts": [], # List for structured prompts
          "final_assembled_prompts": [], # List for final text prompts
          "generated_image_results": [], # List for image gen outcomes
          "llm_call_usage": {}, # To store token usage from various calls
          "cost_summary": PipelineCostSummary().model_dump() if PipelineCostSummary else None # Initialize cost summary
      }
    }
    if mode in ['Custom Mode', 'Task-Specific Mode']:
        pipeline_data["user_inputs"]["branding_elements"] = inputs_data.get('branding_elements') if inputs_data.get('branding_elements') else None
        pipeline_data["user_inputs"]["task_description"] = inputs_data.get('task_description') if inputs_data.get('task_description') else None
        pipeline_data["user_inputs"]["marketing_goals"] = marketing_goals_data
    else: # Easy Mode
        pipeline_data["user_inputs"]["branding_elements"] = None
        pipeline_data["user_inputs"]["task_description"] = None
        pipeline_data["user_inputs"]["marketing_goals"] = None
    return pipeline_data

def parse_and_validate_pipeline_data(pipeline_data: Dict[str, Any]) -> Tuple[bool, str]:
    """Simulates server-side validation of the initial pipeline_data structure."""
    try:
        if not all(k in pipeline_data for k in ["request_details", "user_inputs", "processing_context", "pipeline_settings"]):
            raise ValueError("Missing top-level keys.")
        rd = pipeline_data["request_details"]
        ui = pipeline_data["user_inputs"]
        mode = rd.get("mode")
        platform_info = rd.get("target_platform")
        if not mode or not platform_info or not platform_info.get("name"):
             raise ValueError("Missing mode or target_platform name in request_details.")
        if platform_info.get("name") == PLATFORM_DISPLAY_NAMES[0]: raise ValueError("Invalid target_platform name.")
        if platform_info.get("resolution_details") is not None and not isinstance(platform_info.get("resolution_details"), dict):
             raise ValueError("Invalid target_platform resolution_details format.")

        valid_modes = ["easy_mode", "custom_mode", "task_specific_mode"]
        if mode not in valid_modes: raise ValueError(f"Unknown mode '{mode}'.")
        if mode in ["easy_mode", "custom_mode"] and ui.get("prompt") is None and ui.get("image_reference") is None:
            raise ValueError(f"{mode.replace('_', ' ').title()} requires 'prompt' or 'image_reference'.")
        if mode == "task_specific_mode":
             if not rd.get("task_type"): raise ValueError("Task-specific mode requires 'task_type'.")
             if rd.get("task_type") == TASK_TYPES[0]: raise ValueError("Invalid task_type value.")
        img_ref = ui.get("image_reference")
        if img_ref is not None:
            if not isinstance(img_ref, dict): raise ValueError("image_reference must be an object.")
            if not all(k in img_ref for k in ["filename", "content_type", "size_bytes"]):
                 raise ValueError("image_reference missing required keys.")
        mkt_goals = ui.get("marketing_goals")
        if mode != "easy_mode" and mkt_goals is not None and not isinstance(mkt_goals, dict):
             raise ValueError("marketing_goals must be an object or None.")
        if "render_text" not in ui or not isinstance(ui["render_text"], bool): raise ValueError("render_text missing/invalid.")
        if "apply_branding" not in ui or not isinstance(ui["apply_branding"], bool): raise ValueError("apply_branding missing/invalid.")
        if mode in ["custom_mode", "task_specific_mode"]:
            if "branding_elements" not in ui: raise ValueError("branding_elements missing.")
            if "task_description" not in ui: raise ValueError("task_description missing.")
            if ui.get("branding_elements") is not None and not isinstance(ui["branding_elements"], str): raise ValueError("branding_elements must be string or None.")
            if ui.get("task_description") is not None and not isinstance(ui["task_description"], str): raise ValueError("task_description must be string or None.")
        elif mode == "easy_mode":
            if ui.get("branding_elements") is not None: raise ValueError("branding_elements should be None for Easy mode.")
            if ui.get("task_description") is not None: raise ValueError("task_description should be None for Easy mode.")
        if "creativity_level_selected" not in pipeline_data["pipeline_settings"]: raise ValueError("creativity_level_selected missing.")

        pipeline_data["processing_context"]["initial_json_valid"] = True
        return True, "Server-Side Validation: Pipeline data structure seems valid."
    except Exception as e:
        pipeline_data["processing_context"]["initial_json_valid"] = False
        return False, f"Server-Side Validation Error: {e}"

# --- Image Evaluation Function (from upstream) ---
def perform_image_evaluation(pipeline_data: Dict[str, Any]) -> Tuple[str, Optional[Dict[str, Any]], Optional[Dict[str, Any]], str]:
    """Performs image analysis using VLM, updates pipeline_data."""
    image_ref = pipeline_data.get("user_inputs", {}).get("image_reference")
    analysis_result_dict = None
    status_message = "No image provided for evaluation."
    usage_info = None
    status_code = 'NO_IMAGE'

    if not image_ref:
        pipeline_data["processing_context"]["image_analysis_result"] = None
        return status_message, None, usage_info, status_code

    filename = image_ref.get("filename")
    user_has_provided_instruction = bool(image_ref.get("instruction"))
    content_type = image_ref.get("content_type")
    size = image_ref.get("size_bytes")
    image_content_base64 = image_ref.get("image_content_base64")

    task_type = pipeline_data.get("request_details", {}).get("task_type", "N/A")
    platform = pipeline_data.get("request_details", {}).get("target_platform", {}).get("name", "N/A")
    status_prefix = f"Image '{filename}' ({content_type}, {size} bytes): "

    vlm_prompt_text_parts = [
        "You are an expert visual analyst for F&B marketing. Analyze the provided image objectively.",
        f"The F&B marketing context is: Task='{task_type}', Target Platform='{platform}'.",
    ]
    if user_has_provided_instruction:
        vlm_prompt_text_parts.append("Provide a detailed visual analysis. Identify the `main_subject` concisely. Also describe any `secondary_elements`, the `setting_environment`, the visual `style_mood` of the image, and extract any `extracted_text` visible in the image.")
    else:
        vlm_prompt_text_parts.append("Strictly identify ONLY the `main_subject` of the image. Do not provide analysis for any other fields like secondary elements, setting, style, or extracted text, even if they are part of the response model. Those fields should be omitted or explicitly set to null/None if the model requires them.")
    vlm_prompt_text_parts.append("Focus strictly on what is visually present in the image. Do not infer or add elements not visible. Provide the analysis based on the `ImageAnalysisResult` response model. Be concise and objective.")
    final_vlm_text_prompt = "\n".join(vlm_prompt_text_parts)

    # Determine which client to use (instructor or base)
    client_to_use = instructor_client_img_eval if instructor_client_img_eval and not FORCE_MANUAL_JSON_PARSE else base_llm_client_img_eval
    use_instructor_for_call = bool(instructor_client_img_eval and not FORCE_MANUAL_JSON_PARSE)

    if client_to_use and ImageAnalysisResult:
        print(f"(Attempting VLM call for image '{filename}' using {IMG_EVAL_MODEL_PROVIDER} model: {IMG_EVAL_MODEL_ID}...)")
        try:
            user_content_for_vlm = [{"type": "text", "text": final_vlm_text_prompt}]
            if image_content_base64:
                 user_content_for_vlm.append({"type": "image_url", "image_url": {"url": f"data:{content_type};base64,{image_content_base64}"}})
            else: raise ValueError("Image content (base64) is missing for VLM analysis.")

            messages: List[ChatCompletionMessageParam] = [ # type: ignore
                {"role": "system", "content": "You are an expert visual analyst for F&B marketing. Adhere to the Pydantic response model. Ensure all requested fields are populated appropriately based on the user prompt (e.g., null if not applicable or not requested for minimal analysis)."},
                {"role": "user", "content": user_content_for_vlm} # type: ignore
            ]
            llm_args:Dict[str, Any] = { # Ensure llm_args is typed for clarity
                "model": IMG_EVAL_MODEL_ID, "messages": messages,
                "temperature": 0.2, "max_tokens": 400,
            }
            if use_instructor_for_call:
                llm_args["response_model"] = ImageAnalysisResult

            completion = client_to_use.chat.completions.create(**llm_args)

            if use_instructor_for_call:
                analysis_result_dict = completion.model_dump()
            else: # Manual parse
                raw_content = completion.choices[0].message.content
                json_str = extract_json_from_llm_response(raw_content)
                if not json_str:
                    print(f"  ERROR: Could not extract JSON from VLM response.")
                    print(f"  Raw VLM content: {raw_content}")
                    raise Exception(f"JSON object not found in VLM response. Raw: {raw_content}")
                try:
                    # json_start = raw_content.find('{')
                    # json_end = raw_content.rfind('}') + 1
                    # if json_start != -1 and json_end != -1:
                    #     json_str = raw_content[json_start:json_end]
                    #     parsed_data = json.loads(json_str)
                    #     analysis_result_dict = ImageAnalysisResult(**parsed_data).model_dump()
                    # else: raise json.JSONDecodeError("JSON object not found in response", raw_content,0)
                    parsed_data = json.loads(json_str)
                    analysis_result_dict = ImageAnalysisResult(**parsed_data).model_dump()
                except (json.JSONDecodeError, ValidationError) as parse_err:
                    print(f"  ERROR: Manual JSON parsing/validation failed for VLM response: {parse_err}")
                    print(f"  Extracted JSON string: {json_str}")
                    print(f"  Raw VLM content: {raw_content}")
                    raise Exception(f"VLM response parsing error: {parse_err}") # Re-raise to be caught below

            raw_response_obj = getattr(completion, '_raw_response', completion) # Handle instructor wrapped or direct
            if hasattr(raw_response_obj, 'usage') and raw_response_obj.usage:
                usage_info = raw_response_obj.usage.model_dump()
                print(f"  Token Usage (Image Eval): {usage_info}")
            else: print("  Token usage data not directly available from VLM response object.")
            print(f"  Successfully received and validated Pydantic response from VLM.")
            status_message = status_prefix + "Analysis complete (via VLM)."
            status_code = 'SUCCESS'
        except ValueError as ve: # E.g. missing base64
             print(f"  ERROR preparing VLM call: {ve}")
             status_message = status_prefix + f"Analysis skipped ({ve}). Falling back to simulation."
             analysis_result_dict = simulate_image_evaluation_fallback(user_has_provided_instruction)
             status_code = 'SIMULATED_FALLBACK_PREPARATION_ERROR'
        except (APIConnectionError, RateLimitError, APIStatusError) as api_error:
             print(f"  ERROR: VLM API call failed: {api_error}")
             status_message = status_prefix + f"Analysis failed ({type(api_error).__name__}). Falling back to simulation."
             analysis_result_dict = simulate_image_evaluation_fallback(user_has_provided_instruction)
             status_code = 'API_ERROR'
        except Exception as e:
            if isinstance(e, RetryError): print(f"  ERROR: VLM call failed after retries. Cause: {e.last_attempt.exception()}")
            else: print(f"  ERROR during VLM API call/parsing: {e}")
            print(traceback.format_exc())
            status_message = status_prefix + f"Analysis failed (Error: {e}). Falling back to simulation."
            analysis_result_dict = simulate_image_evaluation_fallback(user_has_provided_instruction)
            status_code = 'API_ERROR_GENERAL' # Or a more specific error if parsing failed
    else:
         print("(VLM client or Pydantic ImageAnalysisResult not configured, using basic simulation for image evaluation)")
         analysis_result_dict = simulate_image_evaluation_fallback(user_has_provided_instruction)
         status_message = status_prefix + "Analysis simulated (No API Key / Library / Model)."
         status_code = 'SIMULATED_NO_API_CONFIG'

    pipeline_data["processing_context"]["image_analysis_result"] = analysis_result_dict
    if usage_info:
        pipeline_data["processing_context"]["llm_call_usage"]["image_eval"] = usage_info
    return status_message, analysis_result_dict, usage_info, status_code

# --- Marketing Strategy Generation (from upstream) ---
def generate_marketing_strategies(pipeline_data: Dict[str, Any], num_strategies: int = 3) -> Tuple[str, Optional[List[Dict]], Optional[Dict], str]:
    """Generates N diverse marketing strategy combinations using a STAGED LLM approach."""
    user_goals = pipeline_data.get("user_inputs", {}).get("marketing_goals")
    user_goals_complete = bool(user_goals and all(user_goals.get(k) for k in ['target_audience', 'objective', 'voice', 'niche']))

    status_message = f"Generating {num_strategies} marketing strategy suggestions..."
    if user_goals_complete: status_message = f"User provided complete marketing goals. Generating {num_strategies} variations..."
    print(status_message)

    task_type = pipeline_data.get("request_details", {}).get("task_type", "N/A")
    user_prompt_input = pipeline_data.get("user_inputs", {}).get("prompt")
    task_description = pipeline_data.get("user_inputs", {}).get("task_description")
    image_analysis = pipeline_data.get("processing_context", {}).get("image_analysis_result")
    image_subject = "N/A"
    if isinstance(image_analysis, dict): image_subject = image_analysis.get("main_subject", "Analysis Failed" if "error" in image_analysis else "N/A")

    task_pools = get_pools_for_task(task_type)
    niche_pool_inspiration = task_pools.get("niche", TASK_GROUP_POOLS["default"]["niche"])

    # Stage 1: Niche Identification
    identified_niches = []
    stage1_status = "Starting Niche Identification..."
    print(f"  Stage 1: {stage1_status}")
    stage1_status_code = 'INIT'
    num_niches_to_find = 3
    user_provided_niche = (user_goals or {}).get('niche')
    usage_info_stage1 = None
    stage1_duration = 0.0 # For latency tracking of this sub-stage

    client_to_use_strat = instructor_client_strategy if instructor_client_strategy and not FORCE_MANUAL_JSON_PARSE else base_llm_client_strategy
    use_instructor_for_strat_call = bool(instructor_client_strategy and not FORCE_MANUAL_JSON_PARSE)

    if user_goals_complete and user_provided_niche:
        identified_niches = [user_provided_niche]
        stage1_status = f"Using user-provided complete goals. Niche fixed to: '{user_provided_niche}'."
        stage1_status_code = 'USER_PROVIDED_COMPLETE'
    elif user_provided_niche:
        identified_niches = [user_provided_niche]
        stage1_status = f"Using user-provided niche: '{user_provided_niche}'. Other goals will be generated."
        stage1_status_code = 'USER_PROVIDED'
    elif client_to_use_strat and RelevantNicheList:
        stage1_call_start_time = time.time()
        try:
            print(f"    (Attempting LLM call for {num_niches_to_find} Niche Identifications via {STRATEGY_MODEL_PROVIDER} model: {STRATEGY_MODEL_ID}...)")
            niche_system_prompt = f"You are an expert F&B market analyst. Your task is to identify a list of {num_niches_to_find} diverse but MOST relevant F&B niches for the given context. Consider the image subject, task description, and task type. Prioritize the most logical fits. Output ONLY the JSON object matching the Pydantic `RelevantNicheList` model containing the list of niche names."
            niche_user_prompt = f"Identify {num_niches_to_find} diverse but relevant F&B niches for this context:\nTask Type: {task_type or 'N/A'}\nTask-Specific Content/Description: {task_description or 'Not Provided'}\nIdentified Image Subject: {image_subject or 'Not Provided / Not Applicable'}\nDetermine the best `relevant_niches` based on the context. Ensure niches are plausible for the image subject."

            llm_args_niche:Dict[str, Any] = {"model": STRATEGY_MODEL_ID, "messages": [{"role": "system", "content": niche_system_prompt}, {"role": "user", "content": niche_user_prompt}], "temperature": 0.4, "max_tokens": 250} # type: ignore

            if use_instructor_for_strat_call:
                llm_args_niche["response_model"] = RelevantNicheList

            completion_niche = client_to_use_strat.chat.completions.create(**llm_args_niche)

            if use_instructor_for_strat_call:
                identified_niches = completion_niche.relevant_niches
            else: # Manual parse
                raw_content_niche = completion_niche.choices[0].message.content
                json_str_niche = extract_json_from_llm_response(raw_content_niche)
                if not json_str_niche:
                    print(f"    ERROR: Could not extract JSON for Niche ID from LLM response.")
                    print(f"    Raw Niche ID content: {raw_content_niche}")
                    raise Exception(f"JSON object not found for niches. Raw: {raw_content_niche}")
                try:
                    # json_start_niche = raw_content_niche.find('{')
                    # json_end_niche = raw_content_niche.rfind('}') + 1
                    # if json_start_niche != -1 and json_end_niche != -1:
                    #     json_str_niche = raw_content_niche[json_start_niche:json_end_niche]
                    #     parsed_data_niche = json.loads(json_str_niche)
                    #     # Assuming parsed_data_niche is like {"relevant_niches": ["niche1", "niche2"]}
                    #     validated_niche_list = RelevantNicheList(**parsed_data_niche)
                    #     identified_niches = validated_niche_list.relevant_niches
                    # else: raise json.JSONDecodeError("JSON object not found for niches", raw_content_niche, 0)
                    parsed_data_niche = json.loads(json_str_niche)
                    # Handle if LLM returns a list directly for relevant_niches
                    if isinstance(parsed_data_niche, list) and "relevant_niches" in RelevantNicheList.model_fields:
                        data_for_pydantic_niche = {"relevant_niches": parsed_data_niche}
                        validated_niche_list = RelevantNicheList(**data_for_pydantic_niche)
                    elif isinstance(parsed_data_niche, dict):
                        validated_niche_list = RelevantNicheList(**parsed_data_niche)
                    else:
                        raise ValidationError(f"Parsed JSON for Niche ID is not a list or dict: {type(parsed_data_niche)}")
                    identified_niches = validated_niche_list.relevant_niches
                except (json.JSONDecodeError, ValidationError) as parse_err_niche:
                    print(f"    ERROR: Manual JSON parsing/validation failed for Niche ID: {parse_err_niche}")
                    print(f"    Extracted Niche ID JSON string: {json_str_niche}")
                    print(f"    Raw Niche ID content: {raw_content_niche}")
                    raise Exception(f"Niche ID response parsing error: {parse_err_niche}")


            raw_response_niche_obj = getattr(completion_niche, '_raw_response', completion_niche)
            if hasattr(raw_response_niche_obj, 'usage') and raw_response_niche_obj.usage: usage_info_stage1 = raw_response_niche_obj.usage.model_dump()
            stage1_status = f"Niches identified via LLM: {identified_niches}"
            stage1_status_code = 'SUCCESS'
        except Exception as e:
            print(f"    ERROR during Niche Identification LLM call: {e}")
            stage1_status = "Niche Identification LLM call failed. Falling back to simulation."
            identified_niches = random.sample(niche_pool_inspiration, min(len(niche_pool_inspiration), random.randint(1,num_niches_to_find)))
            stage1_status_code = 'API_ERROR'
        finally:
            stage1_duration = time.time() - stage1_call_start_time
            if usage_info_stage1: # Log cost for Niche ID if successful
                 _calculate_and_log_cost(pipeline_data, "Strategy - Niche ID", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER,
                                        duration=stage1_duration, usage_info=usage_info_stage1)
            elif stage1_status_code == 'API_ERROR': # Log error for Niche ID if failed
                 _calculate_and_log_cost(pipeline_data, "Strategy - Niche ID", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER,
                                        duration=stage1_duration, usage_info=None,
                                        image_details={"count":0, "error": stage1_status}) # Using image_details to pass error
    else:
        stage1_status = "Niche Identification skipped (Client/Pydantic unavailable or no user niche). Using simulation."
        identified_niches = random.sample(niche_pool_inspiration, min(len(niche_pool_inspiration), random.randint(1,num_niches_to_find)))
        stage1_status_code = 'SIMULATED_NO_API_CONFIG'
        _calculate_and_log_cost(pipeline_data, "Strategy - Niche ID", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER, duration=0, usage_info=None, image_details={"count":0, "error": stage1_status})

    print(f"    Status (Niche ID): {stage1_status}")
    if not identified_niches:
         print("    WARNING: No niches identified or simulated, using default.")
         identified_niches = [random.choice(niche_pool_inspiration)]
         stage1_status += " (Used default niche as fallback)"
    print(f"  Using Niches: {identified_niches} for strategy generation.")

    # Stage 2: Goal Combination Generation
    stage2_status = "Starting Goal Combination Generation..."
    print(f"  Stage 2: {stage2_status}")
    stage2_status_code = 'INIT'
    suggested_strategies_final = [] # List of MarketingGoalSetFinal dicts
    usage_info_stage2 = None
    stage2_duration = 0.0

    if client_to_use_strat and MarketingStrategyOutputStage2 and MarketingGoalSetStage2 and MarketingGoalSetFinal:
        stage2_call_start_time = time.time()
        try:
            print(f"    (Attempting LLM call for Goal Combinations via {STRATEGY_MODEL_PROVIDER} model: {STRATEGY_MODEL_ID}...)")
            system_prompt_stage2 = f"You are an expert F&B Marketing Strategist. Your goal is to generate {num_strategies} diverse and strategically sound marketing goal combinations. For each combination: 1. Select ONE niche from the provided 'Relevant Niches List'. If only one niche is provided (especially if it came directly from the user's complete input), ALL strategies must use that niche. Otherwise, aim to use different niches from the list across the {num_strategies} combinations for diversity. 2. Generate a fitting `target_audience`, `target_objective`, and `target_voice` that logically align with the **chosen niche** for that specific combination and the overall context (task type, image subject). **Handling User Input (Very Important):** - If the user provided a value for audience, objective, or voice (or all of them), treat these as **strong thematic guidelines or a complete foundation**. - If the user provided a COMPLETE set of goals (audience, niche, objective, voice), your task is to generate {num_strategies} insightful VARIATIONS or REFINEMENTS based on this foundation. Each variation should be distinct, strategically sound, and explore different angles while staying true to the user's core intent and the fixed niche. Do NOT just copy the user input verbatim for all strategies unless it's the absolute best fit for one specific variation. - If the user provided only PARTIAL goals, generate values for the missing fields that are thematically consistent with the provided ones and the chosen niche. Ensure the {num_strategies} generated combinations are distinct and make sense. Output ONLY the JSON object matching the `MarketingStrategyOutputStage2` model containing a list of exactly {num_strategies} `MarketingGoalSetStage2` objects. The keys in each object inside the list MUST be `target_audience`, `target_objective`, and `target_voice`."
            user_goals_guidance_text = ""
            if user_goals_complete: user_goals_guidance_text = f"The user has provided a COMPLETE set of marketing goals as a foundation:\nUser's Target Audience: {user_goals.get('target_audience')}\nUser's Chosen Niche: {user_goals.get('niche')} (All your generated strategies MUST use this niche)\nUser's Target Objective: {user_goals.get('objective')}\nUser's Desired Voice: {user_goals.get('voice')}\nYour task is to generate {num_strategies} distinct and insightful VARIATIONS or REFINEMENTS based on this solid foundation. Explore different angles while adhering to the user's core intent for all four components."
            else: user_goals_guidance_text = f"User-Provided Goals (Use these as strong thematic guidelines for the fields they provided; generate appropriate values for missing fields):\nAudience: {(user_goals or {}).get('target_audience') or 'Not Provided'}\nNiche: {user_provided_niche or 'Not Provided (refer to Relevant Niches List below)'}\nObjective: {(user_goals or {}).get('objective') or 'Not Provided'}\nVoice: {(user_goals or {}).get('voice') or 'Not Provided'}"
            user_prompt_context_stage2 = f"Generate {num_strategies} diverse marketing strategy combinations for the following F&B task.\nTask Type: {task_type or 'N/A'}\nTarget Platform: {pipeline_data.get('request_details', {}).get('target_platform', {}).get('name', 'N/A')}\nUser's General Prompt: {user_prompt_input or 'Not Provided'}\nTask-Specific Content/Description: {task_description or 'Not Provided'}\nIdentified Image Subject: {image_subject or 'Not Provided / Not Applicable'}\n\n**Relevant Niches List (One niche from this list should be used for each strategy. If the user provided a complete set of goals including a niche, that niche is fixed and MUST be used for all strategies):** {identified_niches}\n\n{user_goals_guidance_text}\n\nGenerate {num_strategies} complete, diverse, and strategically relevant combinations. Each item in the 'strategies' list of the output JSON should have keys: `target_audience`, `target_objective`, `target_voice`. Adhere strictly to the output format."

            llm_args_goals:Dict[str, Any] = {"model": STRATEGY_MODEL_ID, "messages": [{"role": "system", "content": system_prompt_stage2}, {"role": "user", "content": user_prompt_context_stage2}], "temperature": 0.7, "max_tokens": 1500} # type: ignore
            if use_instructor_for_strat_call:
                llm_args_goals["response_model"] = MarketingStrategyOutputStage2

            completion_stage2 = client_to_use_strat.chat.completions.create(**llm_args_goals)

            temp_strategies_stage2 = []
            if use_instructor_for_strat_call:
                temp_strategies_stage2 = [s.model_dump() for s in completion_stage2.strategies]
            else: # Manual parse
                raw_content_goals = completion_stage2.choices[0].message.content
                json_str_goals = extract_json_from_llm_response(raw_content_goals)
                if not json_str_goals:
                    print(f"    ERROR: Could not extract JSON for Goal Gen from LLM response.")
                    print(f"    Raw Goal Gen content: {raw_content_goals}")
                    raise Exception(f"JSON object not found for strategies. Raw: {raw_content_goals}")
                try:
                    # json_start_goals = raw_content_goals.find('{')
                    # json_end_goals = raw_content_goals.rfind('}') + 1
                    # if json_start_goals != -1 and json_end_goals != -1:
                    #     json_str_goals = raw_content_goals[json_start_goals:json_end_goals]
                    #     parsed_data_goals = json.loads(json_str_goals) # Expects {"strategies": [...]}
                    #     validated_goals_output = MarketingStrategyOutputStage2(**parsed_data_goals)
                    #     temp_strategies_stage2 = [s.model_dump() for s in validated_goals_output.strategies]
                    # else: raise json.JSONDecodeError("JSON object not found for strategies", raw_content_goals, 0)
                    parsed_data_goals = json.loads(json_str_goals)

                    data_for_pydantic_validation = {}

                    # This block handles both when LLM returns a list directly,
                    # or a dict like {"strategies": [...]}, and renames keys.
                    actual_list_of_strategy_dicts = []

                    if isinstance(parsed_data_goals, list):
                        # LLM returned a list of strategies directly.
                        # Need to rename keys within each item.
                        transformed_list = []
                        for item in parsed_data_goals:
                            if isinstance(item, dict):
                                transformed_item = {
                                    "target_audience": item.get("audience", item.get("target_audience")),
                                    "target_objective": item.get("objective", item.get("target_objective")),
                                    "target_voice": item.get("voice", item.get("target_voice"))
                                }
                                # Keep only non-None values to avoid Pydantic validation errors if a key was truly missing
                                transformed_list.append({k: v for k, v in transformed_item.items() if v is not None})
                            else:
                                transformed_list.append(item) # Should not happen if LLM follows instructions
                        data_for_pydantic_validation = {"strategies": transformed_list}

                    elif isinstance(parsed_data_goals, dict) and "strategies" in parsed_data_goals and isinstance(parsed_data_goals["strategies"], list):
                        # LLM returned the expected {"strategies": [...]} structure, but keys inside might be wrong.
                        transformed_list = []
                        for item in parsed_data_goals["strategies"]:
                            if isinstance(item, dict):
                                transformed_item = {
                                    "target_audience": item.get("audience", item.get("target_audience")),
                                    "target_objective": item.get("objective", item.get("target_objective")),
                                    "target_voice": item.get("voice", item.get("target_voice"))
                                }
                                transformed_list.append({k: v for k, v in transformed_item.items() if v is not None})
                            else:
                                transformed_list.append(item)
                        data_for_pydantic_validation = {"strategies": transformed_list}
                    else: # Did not get a list or a dict with a "strategies" list
                        raise ValidationError(f"Parsed JSON for Goal Gen is not a list or a dict with a 'strategies' list: {type(parsed_data_goals)}")

                    validated_goals_output = MarketingStrategyOutputStage2(**data_for_pydantic_validation)
                    temp_strategies_stage2 = [s.model_dump() for s in validated_goals_output.strategies]

                except (json.JSONDecodeError, ValidationError) as parse_err_goals:
                    print(f"    ERROR: Manual JSON parsing/validation failed for Goal Gen: {parse_err_goals}")
                    print(f"    Extracted Goal Gen JSON string: {json_str_goals}")
                    print(f"    Raw Goal Gen content: {raw_content_goals}")
                    raise Exception(f"Goal Gen response parsing error: {parse_err_goals}")

            for i, strategy_s2_dict in enumerate(temp_strategies_stage2):
                assigned_niche = user_provided_niche if user_goals_complete else identified_niches[i % len(identified_niches)]
                final_strat_dict = {**strategy_s2_dict, 'target_niche': assigned_niche}
                try:
                    suggested_strategies_final.append(MarketingGoalSetFinal(**final_strat_dict).model_dump())
                except Exception as pydantic_error: print(f"    WARNING: Failed to validate final strategy structure: {final_strat_dict}. Error: {pydantic_error}")
            raw_response_strat_obj = getattr(completion_stage2, '_raw_response', completion_stage2)
            if hasattr(raw_response_strat_obj, 'usage') and raw_response_strat_obj.usage: usage_info_stage2 = raw_response_strat_obj.usage.model_dump()
            if len(suggested_strategies_final) != num_strategies: print(f"    WARNING: LLM generated {len(suggested_strategies_final)} valid strategies, but {num_strategies} were requested.")
            else: print(f"    Successfully received and validated {len(suggested_strategies_final)} marketing strategies from LLM.")
            stage2_status = "Goal combinations generated successfully."
            stage2_status_code = 'SUCCESS'
        except Exception as e:
            print(f"    ERROR during LLM API call for goal combinations: {e}\n{traceback.format_exc()}")
            stage2_status = f"Goal Combination generation failed. Falling back to simulation."
            suggested_strategies_final = simulate_marketing_strategy_fallback_staged(user_goals, identified_niches, task_type, num_strategies)
            stage2_status_code = 'API_ERROR'
        finally:
            stage2_duration = time.time() - stage2_call_start_time
            if usage_info_stage2: # Log cost for Goal Gen if successful
                _calculate_and_log_cost(pipeline_data, "Strategy - Goal Gen", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER,
                                        duration=stage2_duration, usage_info=usage_info_stage2)
            elif stage2_status_code == 'API_ERROR': # Log error for Goal Gen if failed
                _calculate_and_log_cost(pipeline_data, "Strategy - Goal Gen", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER,
                                        duration=stage2_duration, usage_info=None,
                                        image_details={"count":0, "error": stage2_status})
    else:
         suggested_strategies_final = simulate_marketing_strategy_fallback_staged(user_goals, identified_niches, task_type, num_strategies)
         stage2_status = "Goal combinations simulated (Client/Pydantic unavailable)."
         stage2_status_code = 'SIMULATED_NO_API_CONFIG'
         _calculate_and_log_cost(pipeline_data, "Strategy - Goal Gen", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER, duration=0, usage_info=None, image_details={"count":0, "error": stage2_status})

    print(f"  Stage 2: {stage2_status}")

    pipeline_data["processing_context"]["suggested_marketing_strategies"] = suggested_strategies_final
    if usage_info_stage1: pipeline_data["processing_context"]["llm_call_usage"]["strategy_niche_id"] = usage_info_stage1
    if usage_info_stage2: pipeline_data["processing_context"]["llm_call_usage"]["strategy_goal_gen"] = usage_info_stage2
    combined_usage = {**(usage_info_stage1 or {}), **(usage_info_stage2 or {})} # Simple merge for now

    overall_status_code = 'UNKNOWN_STRATEGY_ERROR'
    if stage1_status_code == 'API_ERROR' or stage2_status_code == 'API_ERROR': overall_status_code = 'API_ERROR'
    elif stage1_status_code == 'SIMULATED_NO_API_CONFIG' or stage2_status_code == 'SIMULATED_NO_API_CONFIG': overall_status_code = 'SIMULATED_NO_API_CONFIG'
    elif stage1_status_code in ['USER_PROVIDED', 'USER_PROVIDED_COMPLETE', 'SUCCESS'] and stage2_status_code == 'SUCCESS': overall_status_code = 'SUCCESS'

    final_status_message = f"Marketing strategies generated. (Niche ID: {stage1_status.split('.')[0]}; Goal Gen: {stage2_status.split('.')[0]})"
    if user_goals_complete: final_status_message = f"Marketing strategy variations generated based on user's complete goals. (Niche ID: {stage1_status.split('.')[0]}; Goal Gen: {stage2_status.split('.')[0]})"
    return final_status_message, suggested_strategies_final, combined_usage, overall_status_code


# --- Style Guider Function (from downstream) ---
def generate_style_guidance_for_strategies(pipeline_data: Dict[str, Any]) -> Tuple[Optional[List[Dict[str, Any]]], Optional[Dict[str, int]], Optional[str]]:
    """Generates N distinct style guidance sets for N marketing strategies."""
    if not StyleGuidanceList or not StyleGuidance: return None, None, "Error: StyleGuider Pydantic model not available."

    strategies = pipeline_data.get("processing_context", {}).get("suggested_marketing_strategies", [])
    if not strategies: return [], None, "No marketing strategies provided to Style Guider."

    num_strategies = len(strategies)
    task_type = pipeline_data.get("request_details", {}).get("task_type", "N/A")
    creativity_level = pipeline_data.get("pipeline_settings", {}).get("creativity_level_selected", 2)
    image_analysis = pipeline_data.get("processing_context", {}).get("image_analysis_result")
    image_subject_from_analysis = image_analysis.get("main_subject") if isinstance(image_analysis, dict) else None
    user_prompt_original = pipeline_data.get("user_inputs", {}).get("prompt")

    client_to_use_sg = instructor_client_style_guide if instructor_client_style_guide and not FORCE_MANUAL_JSON_PARSE else base_llm_client_style_guide
    use_instructor_for_sg_call = bool(instructor_client_style_guide and not FORCE_MANUAL_JSON_PARSE and STYLE_GUIDER_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS)

    if not client_to_use_sg: return None, None, "LLM Client for Style Guider not available."

    system_prompt_sg = get_system_prompt(
        creativity_level=creativity_level, task_type=task_type, use_instructor_parsing=use_instructor_for_sg_call,
        has_reference=False, has_instruction=False, render_text_flag=False, apply_branding_flag=False,
        is_style_guider_prompt=True, num_strategies_for_style_guider=num_strategies,
        target_model_family=STYLE_GUIDER_MODEL_PROVIDER.lower() # Pass model family for potential Qwen specifics
    )
    user_prompt_sg = get_style_guider_user_prompt(strategies, task_type, image_subject_from_analysis, user_prompt_original, num_strategies, use_instructor_for_sg_call)

    llm_args_sg: Dict[str, Any] = { # type: ignore
        "model": STYLE_GUIDER_MODEL_ID, "messages": [{"role": "system", "content": system_prompt_sg}, {"role": "user", "content": user_prompt_sg}],
        "temperature": 0.8, "max_tokens": 1500 * num_strategies, "extra_body": {}
    }
    if use_instructor_for_sg_call and StyleGuidanceList:
        llm_args_sg["response_model"] = StyleGuidanceList

    # OpenRouter provider preference (example, not used by default here)
    # if STYLE_GUIDER_MODEL_PROVIDER == "OpenRouter" and PREFERRED_PROVIDERS_LIST_FOR_STYLE_GUIDER:
    #     llm_args_sg["extra_body"] = {"provider": {"only": PREFERRED_PROVIDERS_LIST_FOR_STYLE_GUIDER, "allow_fallbacks": False}}

    usage_info_sg = None; error_details_sg = None; style_guidance_list_data = None
    try:
        print(f"\n--- Generating Style Guidance for {num_strategies} strategies (Creativity: {creativity_level}) using {STYLE_GUIDER_MODEL_PROVIDER} model: {STYLE_GUIDER_MODEL_ID} ---")
        # Determine effective client and parsing strategy
        effective_client_sg = client_to_use_sg
        actually_use_instructor_parsing_sg = use_instructor_for_sg_call

        if use_instructor_for_sg_call and STYLE_GUIDER_MODEL_ID in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS:
            print(f"   Model {STYLE_GUIDER_MODEL_ID} is problematic with instructor tool mode. Forcing manual parse for this call.")
            actually_use_instructor_parsing_sg = False
            if "response_model" in llm_args_sg: del llm_args_sg["response_model"]
            if "tools" in llm_args_sg: del llm_args_sg["tools"]
            if "tool_choice" in llm_args_sg: del llm_args_sg["tool_choice"]
            # Use base client if instructor client was specifically for tool mode that's now bypassed
            effective_client_sg = base_llm_client_style_guide if base_llm_client_style_guide else client_to_use_sg


        completion_sg = effective_client_sg.chat.completions.create(**llm_args_sg) # type: ignore

        if actually_use_instructor_parsing_sg:
            style_guidance_list_data = [item.model_dump() for item in completion_sg.style_guidance_sets]
        else: # Manual parse
            raw_content_sg = completion_sg.choices[0].message.content
            json_str_sg = extract_json_from_llm_response(raw_content_sg)
            if not json_str_sg: # Check if extraction failed
                error_details_sg = f"Style Guider JSON extraction failed. Could not find JSON in response.\nRaw Content: {raw_content_sg}"
                # This will be caught by the outer except block
                raise Exception(error_details_sg)
            try:
                parsed_json_sg = json.loads(json_str_sg)
                # Handle if LLM returns a list directly for style_guidance_sets
                data_for_pydantic_sg_validation = {}
                if isinstance(parsed_json_sg, list) and "style_guidance_sets" in StyleGuidanceList.model_fields:
                    data_for_pydantic_sg_validation = {"style_guidance_sets": parsed_json_sg}
                elif isinstance(parsed_json_sg, dict): # Expected case if LLM follows instructions for {"style_guidance_sets": [...]}
                    data_for_pydantic_sg_validation = parsed_json_sg
                else:
                    raise ValidationError(f"Parsed JSON for Style Guidance is not a list or dict as expected: {type(parsed_json_sg)}")

                validated_model_sg = StyleGuidanceList(**data_for_pydantic_sg_validation)
                style_guidance_list_data = [item.model_dump() for item in validated_model_sg.style_guidance_sets]
            except (json.JSONDecodeError, ValidationError) as err_sg_parse:
                error_details_sg = f"Style Guider JSON/Pydantic error: {err_sg_parse}\nExtracted JSON: {json_str_sg}\nRaw: {raw_content_sg}"
                raise Exception(error_details_sg)

        print(f"✅ Successfully generated {len(style_guidance_list_data)} style guidance sets.")
        raw_response_sg_obj = getattr(completion_sg, '_raw_response', completion_sg)
        if hasattr(raw_response_sg_obj, 'usage') and raw_response_sg_obj.usage:
            usage_info_sg = raw_response_sg_obj.usage.model_dump()
            print(f"  Token Usage (Style Guider): {usage_info_sg}")
        else: print("  Token usage data not available for Style Guider.")
        pipeline_data["processing_context"]["style_guidance_sets"] = style_guidance_list_data
        if usage_info_sg: pipeline_data["processing_context"]["llm_call_usage"]["style_guider"] = usage_info_sg
        return style_guidance_list_data, usage_info_sg, None
    except (APIConnectionError, RateLimitError, APIStatusError) as api_err_sg: error_details_sg = f"Style Guider API call error: {api_err_sg}"
    except Exception as e_sg:
        # If error_details_sg was already set (e.g. by extraction failure), use it. Otherwise, format the current exception.
        if not error_details_sg:
            error_details_sg = f"Style Guider general error: {e_sg}\n{traceback.format_exc()}"
    print(f"❌ ERROR (Style Guider): {error_details_sg}")
    return None, None, error_details_sg


# --- Creative Expert Agent Function (from downstream) ---
def generate_image_prompt_for_strategy(
    pipeline_data: Dict[str, Any],
    strategy: Dict[str, Any], # MarketingGoalSetFinal dict
    strategy_index: int,
    style_guidance_item: Optional[StyleGuidance] # Pydantic object
) -> Tuple[Optional[Dict[str, Any]], Optional[Dict[str, int]], Optional[str]]:
    """Generates a structured visual concept for a specific marketing strategy."""
    if not ImageGenerationPrompt or not VisualConceptDetails:
        return None, None, "Error: Pydantic models for Creative Expert not available."

    creativity_level = pipeline_data.get("pipeline_settings", {}).get("creativity_level_selected", 2)
    request_details = pipeline_data.get("request_details", {})
    user_inputs = pipeline_data.get("user_inputs", {})
    processing_context = pipeline_data.get("processing_context", {})

    task_type = request_details.get("task_type", "N/A")
    platform_info = request_details.get("target_platform", {})
    platform_name = platform_info.get("name", "N/A")
    # Aspect ratio for prompt text (not API size)
    aspect_ratio_for_prompt_text = map_to_supported_aspect_ratio_for_prompt(platform_info.get("resolution_details", {}).get("aspect_ratio", "1:1"))


    user_prompt_original = user_inputs.get("prompt")
    image_reference = user_inputs.get("image_reference") # This is a dict
    branding_elements = user_inputs.get("branding_elements")
    task_description = user_inputs.get("task_description")
    render_text_flag = user_inputs.get("render_text", True) # Default to True if missing
    apply_branding_flag = user_inputs.get("apply_branding", True) # Default to True if missing

    image_analysis = processing_context.get("image_analysis_result") # This is a dict

    has_image_reference = image_reference is not None
    image_instruction = image_reference.get("instruction") if has_image_reference else None
    has_instruction_flag = bool(image_instruction)
    saved_image_filename = image_reference.get("filename") if has_image_reference else None # Original filename
    image_subject_from_analysis = image_analysis.get("main_subject") if isinstance(image_analysis, dict) else None
    is_default_edit_case = has_image_reference and not has_instruction_flag

    client_to_use_ce = instructor_client_creative_expert if instructor_client_creative_expert and not FORCE_MANUAL_JSON_PARSE else base_llm_client_creative_expert
    use_instructor_for_ce_call = bool(instructor_client_creative_expert and not FORCE_MANUAL_JSON_PARSE and CREATIVE_EXPERT_MODEL_ID not in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS)

    if not client_to_use_ce: return None, None, "LLM Client for Creative Expert not available."

    system_prompt_ce = get_system_prompt(
        creativity_level, task_type, use_instructor_for_ce_call, has_image_reference, has_instruction_flag,
        render_text_flag, apply_branding_flag, platform_name,
        is_style_guider_prompt=False, # This is for Creative Expert
        target_model_family=CREATIVE_EXPERT_MODEL_PROVIDER.lower()
    )
    user_prompt_ce_parts = get_user_prompt_parts(
        platform_name, aspect_ratio_for_prompt_text, strategy, task_type, user_prompt_original,
        task_description, branding_elements, render_text_flag, apply_branding_flag,
        has_image_reference, saved_image_filename, image_subject_from_analysis,
        image_instruction, use_instructor_for_ce_call, is_default_edit_case, style_guidance_item
    )
    final_user_prompt_ce = "\n".join(user_prompt_ce_parts)

    llm_args_ce: Dict[str, Any] = { # type: ignore
        "model": CREATIVE_EXPERT_MODEL_ID,
        "messages": [{"role": "system", "content": system_prompt_ce}, {"role": "user", "content": final_user_prompt_ce}],
        "temperature": 0.7, "max_tokens": 5000, "extra_body": {}
    }
    if creativity_level == 1: llm_args_ce["temperature"] = 0.4
    elif creativity_level == 3: llm_args_ce["temperature"] = 0.9

    if use_instructor_for_ce_call and ImageGenerationPrompt:
        llm_args_ce["response_model"] = ImageGenerationPrompt

    usage_info_ce = None; error_details_ce = None; prompt_data_ce = None; raw_response_content_ce = None
    try:
        print(f"\n--- Generating structured prompt for Strategy {strategy_index} (Creativity: {creativity_level}) using {CREATIVE_EXPERT_MODEL_PROVIDER} model: {CREATIVE_EXPERT_MODEL_ID} ---")
        effective_client_ce = client_to_use_ce
        actually_use_instructor_parsing_ce = use_instructor_for_ce_call

        if use_instructor_for_ce_call and CREATIVE_EXPERT_MODEL_ID in INSTRUCTOR_TOOL_MODE_PROBLEM_MODELS:
            print(f"   Model {CREATIVE_EXPERT_MODEL_ID} is problematic with instructor tool mode. Forcing manual parse for this call.")
            actually_use_instructor_parsing_ce = False
            if "response_model" in llm_args_ce: del llm_args_ce["response_model"]
            if "tools" in llm_args_ce: del llm_args_ce["tools"]
            if "tool_choice" in llm_args_ce: del llm_args_ce["tool_choice"]
            effective_client_ce = base_llm_client_creative_expert if base_llm_client_creative_expert else client_to_use_ce

        completion_ce = effective_client_ce.chat.completions.create(**llm_args_ce) # type: ignore

        if actually_use_instructor_parsing_ce:
            prompt_data_ce = completion_ce.model_dump()
        else: # Manual parse
            raw_response_content_ce = completion_ce.choices[0].message.content
            json_string_ce = extract_json_from_llm_response(raw_response_content_ce)
            if not json_string_ce:
                error_details_ce = f"Creative Expert JSON extraction failed.\nRaw: {raw_response_content_ce}"
                raise Exception(error_details_ce)
            try:
                parsed_json_ce = json.loads(json_string_ce)

                if 'visual_concept' in parsed_json_ce and \
                   isinstance(parsed_json_ce.get('visual_concept'), dict) and \
                   'promotional_text_visuals' in parsed_json_ce['visual_concept'] and \
                   isinstance(parsed_json_ce['visual_concept']['promotional_text_visuals'], dict):

                    ptv_dict = parsed_json_ce['visual_concept']['promotional_text_visuals']
                    description_parts = []
                    for ptv_key, ptv_value in ptv_dict.items():
                        formatted_ptv_key = ptv_key.replace('_', ' ').capitalize()
                        if isinstance(ptv_value, list):
                            item_strs = []
                            for item in ptv_value:
                                if isinstance(item, dict) and 'type' in item and 'text' in item: # Handle known list of dicts
                                    item_strs.append(f"{item.get('type', '').capitalize()}: \"{item.get('text', '')}\"")
                                elif isinstance(item, dict): # Handle other dicts in list
                                    item_strs.append(json.dumps(item))
                                else:
                                    item_strs.append(str(item))
                            description_parts.append(f"{formatted_ptv_key}: [{'; '.join(item_strs)}]")
                        elif isinstance(ptv_value, dict): # Handle nested dict value
                            description_parts.append(f"{formatted_ptv_key}: {json.dumps(ptv_value)}")
                        else: # Handle simple value
                            description_parts.append(f"{formatted_ptv_key}: {str(ptv_value)}")

                    if description_parts:
                        parsed_json_ce['visual_concept']['promotional_text_visuals'] = "; ".join(description_parts) + "."
                    else:
                        # Fallback if dict was empty or formatting yielded no parts
                        parsed_json_ce['visual_concept']['promotional_text_visuals'] = json.dumps(ptv_dict)

                validated_model_ce = ImageGenerationPrompt(**parsed_json_ce)
                prompt_data_ce = validated_model_ce.model_dump()
            except (json.JSONDecodeError, ValidationError) as val_err_ce:
                error_details_ce = f"Creative Expert Pydantic/JSON validation error: {val_err_ce}\nExtracted JSON: {json_string_ce}\nRaw: {raw_response_content_ce}"
                raise Exception(error_details_ce)

        if prompt_data_ce is None: raise Exception("Failed to obtain structured data from Creative Expert.")

        # Post-processing based on flags (important for both instructor and manual parse)
        vc_ce = prompt_data_ce.get("visual_concept", {})
        if is_default_edit_case and vc_ce.get("main_subject") is not None: vc_ce["main_subject"] = None
        if not render_text_flag and vc_ce.get("promotional_text_visuals") is not None: vc_ce["promotional_text_visuals"] = None
        if not apply_branding_flag and vc_ce.get("branding_visuals") is not None: vc_ce["branding_visuals"] = None
        prompt_data_ce["visual_concept"] = vc_ce # Ensure updated vc is part of prompt_data_ce
        prompt_data_ce['source_strategy_index'] = strategy_index # Add index

        print(f"✅ Successfully generated structured prompt object for Strategy {strategy_index}.")
        raw_response_ce_obj = getattr(completion_ce, '_raw_response', completion_ce)
        if hasattr(raw_response_ce_obj, 'usage') and raw_response_ce_obj.usage:
            usage_info_ce = raw_response_ce_obj.usage.model_dump()
            print(f"  Token Usage (Creative Expert - Strategy {strategy_index}): {usage_info_ce}")
        else: print("  Token usage data not available for Creative Expert.")
        return prompt_data_ce, usage_info_ce, None
    except (APIConnectionError, RateLimitError, APIStatusError) as api_error_ce: error_details_ce = f"Creative Expert API call error: {api_error_ce}"
    except Exception as e_ce:
        if not error_details_ce: # If not already set by a more specific parsing error
            error_details_ce = f"Creative Expert general error: {e_ce}\n{traceback.format_exc()}"
    if not use_instructor_for_ce_call and raw_response_content_ce and "Raw:" not in error_details_ce :
        error_details_ce += f"\nRaw Content (Creative Expert): {raw_response_content_ce}"
    print(f"❌ ERROR (Creative Expert - Strategy {strategy_index}): {error_details_ce}")
    return None, None, error_details_ce


# --- Prompt Assembly Function (from downstream) ---
def assemble_final_prompt(structured_prompt_data: Dict[str, Any], user_inputs: Dict[str, Any], platform_aspect_ratio: str) -> str:
    """Assembles the final text prompt string from the structured visual concept details."""
    if not structured_prompt_data or "visual_concept" not in structured_prompt_data:
        return "Error: Invalid structured prompt data for assembly."
    vc = structured_prompt_data["visual_concept"]
    image_reference = user_inputs.get("image_reference")
    has_reference = image_reference is not None
    has_instruction = has_reference and image_reference.get("instruction")
    instruction_text = image_reference.get("instruction", "") if has_instruction else ""
    supported_aspect_ratio_for_prompt = map_to_supported_aspect_ratio_for_prompt(platform_aspect_ratio)
    is_default_edit_scenario = has_reference and not has_instruction and vc.get("main_subject") is None # Check if main_subject was correctly omitted

    final_prompt_str = ""
    if is_default_edit_scenario:
        print("   (Assembling simplified prompt for default image edit - preserving subject)")
        prefix = "Edit the provided image. Preserve the main subject exactly as it is in the original image. Modify only the surrounding context (background, lighting, style, composition, etc.) to match this description: "
        context_parts = [
            vc.get("composition_and_framing"), f"Background: {vc.get('background_environment')}",
            f"Foreground elements: {vc.get('foreground_elements')}" if vc.get("foreground_elements") else None,
            f"Lighting & Mood: {vc.get('lighting_and_mood')}", f"Color Palette: {vc.get('color_palette')}",
            f"Visual Style: {vc.get('visual_style')}",
            f"Textures & Details: {vc.get('texture_and_details')}" if vc.get("texture_and_details") else None,
            f"Promotional Text Visuals: {vc.get('promotional_text_visuals')}" if user_inputs.get("render_text", False) and vc.get("promotional_text_visuals") else None,
            f"Branding Visuals: {vc.get('branding_visuals')}" if user_inputs.get("apply_branding", False) and vc.get("branding_visuals") else None,
            f"Avoid the following elements: {vc.get('negative_elements')}" if vc.get("negative_elements") else None,
        ]
        context_description = " ".join(filter(None, context_parts))
        final_prompt_str = f"{prefix}{context_description} IMPORTANT: Ensure the image strictly adheres to a {supported_aspect_ratio_for_prompt} aspect ratio."
    else:
        print("   (Assembling full prompt for generation or instructed edit)")
        core_description_parts = [
            vc.get("main_subject"), vc.get("composition_and_framing"),
            f"Background: {vc.get('background_environment')}",
            f"Foreground elements: {vc.get('foreground_elements')}" if vc.get("foreground_elements") else None,
            f"Lighting & Mood: {vc.get('lighting_and_mood')}", f"Color Palette: {vc.get('color_palette')}",
            f"Visual Style: {vc.get('visual_style')}",
            f"Textures & Details: {vc.get('texture_and_details')}" if vc.get("texture_and_details") else None,
            f"Promotional Text Visuals: {vc.get('promotional_text_visuals')}" if user_inputs.get("render_text", False) and vc.get("promotional_text_visuals") else None,
            f"Branding Visuals: {vc.get('branding_visuals')}" if user_inputs.get("apply_branding", False) and vc.get("branding_visuals") else None,
            f"Avoid the following elements: {vc.get('negative_elements')}" if vc.get("negative_elements") else None,
        ]
        core_description = " ".join(filter(None, core_description_parts))
        prefix = ""
        if has_reference and has_instruction: prefix = f"Based on the provided reference image, modify it according to the user instruction '{instruction_text}' to achieve the following detailed visual concept: "
        final_prompt_str = f"{prefix}{core_description} IMPORTANT: Ensure the image strictly adheres to a {supported_aspect_ratio_for_prompt} aspect ratio."
    return final_prompt_str

# --- Image Generation Function (from downstream) ---
def generate_image(
    final_prompt: str,
    platform_aspect_ratio: str, # Renamed from aspect_ratio to match calling context
    client: OpenAI,
    run_directory: str,
    strategy_index: int,
    reference_image_path: Optional[str] = None,
    image_quality_setting: str = "medium" # Default to medium for gpt-image-1
) -> Tuple[str, Optional[str], Optional[int]]: # Returns (status, url_or_filepath, prompt_tokens_for_image_gen)
    """
    Generates or edits an image using the OpenAI Images API (model specified by IMAGE_GENERATION_MODEL)
    via the specified client. Uses client.images.edit if reference_image_path is provided,
    Otherwise, uses client.images.generate.

    Args:
        final_prompt: The assembled text prompt string describing the desired outcome or edit.
        aspect_ratio: The aspect ratio string ('1:1', '9:16', '16:9', '2:3', '3:4').
        client: The initialized OpenAI client.
        run_directory: The path to the directory where outputs for this run are saved.
        strategy_index: The index of the current strategy (for filename).
        reference_image_path: Optional path to the reference image for editing.

    Returns:
        A tuple containing:
          - status: "success" or "error"
          - url_or_filepath: The image URL (for generation) or local file path (for edits)
                             if successful, or an error message string.
    """

    prompt_tokens_for_image_gen = len(final_prompt.split()) # Simple estimation

    if not client:
        return "error", "Image generation client not available.", prompt_tokens_for_image_gen
    if not final_prompt or final_prompt.startswith("Error:"):
         return "error", f"Invalid final prompt provided: {final_prompt}", prompt_tokens_for_image_gen
    if not run_directory or not os.path.isdir(run_directory):
         return "error", f"Invalid run_directory provided: {run_directory}", prompt_tokens_for_image_gen

    try:
        image_api_size = map_aspect_ratio_to_size_for_api(platform_aspect_ratio)
        if not image_api_size:
            return "error", f"Unsupported aspect ratio '{platform_aspect_ratio}' for image API.", prompt_tokens_for_image_gen

        response: Optional[ImagesResponse] = None
        operation_type = "generation"


        if reference_image_path and os.path.exists(reference_image_path):
            if IMAGE_GENERATION_MODEL_ID:
                operation_type = "editing"
                print(f"--- Calling Image Editing API {IMAGE_GENERATION_MODEL_ID} with reference image ---")
                print(f"   Reference Image: {reference_image_path}")
                try:
                    with open(reference_image_path, "rb") as image_file:
                        response = client.images.edit(
                            model=IMAGE_GENERATION_MODEL_ID,
                            image=image_file,
                            prompt=final_prompt,
                            n=1,
                            size=image_api_size,
                            quality=image_quality_setting
                        )
                except FileNotFoundError:
                 return "error", f"Reference image not found at path: {reference_image_path}", prompt_tokens_for_image_gen
                except Exception as file_err:
                     return "error", f"Error opening reference image: {file_err}", prompt_tokens_for_image_gen
        else: # No reference image path, or path was invalid
            operation_type = "generation"
            if reference_image_path: # Path provided but file not found
                 print(f"⚠️ Warning: Reference image path provided but file not found: {reference_image_path}. Proceeding with generation.")
            print(f"--- Calling Image Generation API ({IMAGE_GENERATION_MODEL_ID}) ---")

            generate_params: Dict[str, Any] = {
                "model": IMAGE_GENERATION_MODEL_ID,
                "prompt": final_prompt,
                "size": image_api_size,
                "n": 1,
                "quality": image_quality_setting
            }

            response = client.images.generate(**generate_params)

        # --- Process Response ---
        if response and response.data and len(response.data) > 0:
            image_data = response.data[0]
            saved_filepath = None

            if image_data.b64_json:
                print(f"✅ Image {operation_type} successful (received base64 data).")
                try:
                    image_bytes = base64.b64decode(image_data.b64_json)
                    timestamp_img = datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')

                    if operation_type == "editing":
                        local_filename = f"edited_image_strategy_{strategy_index}.png"
                    else: # Covers "generation" and "generation_with_reference_in_prompt"
                        local_filename = f"generated_image_strategy_{strategy_index}.png"

                    saved_filepath = os.path.join(run_directory, local_filename)
                    with open(saved_filepath, "wb") as f:
                        f.write(image_bytes)
                    print(f"   Saved image to: {saved_filepath}")

                    return "success", saved_filepath, prompt_tokens_for_image_gen
                except Exception as decode_save_err:
                    print(f"❌ Error decoding/saving base64 image: {decode_save_err}")
                    return "error", f"Error processing base64 response: {decode_save_err}", prompt_tokens_for_image_gen
            elif image_data.url: # Fallback if b64_json wasn't requested or provided (should be rare now)
                print(f"✅ Image {operation_type} successful (received URL). Downloading...")
                image_url = image_data.url
                try:
                    img_response_download = requests.get(image_url, stream=True, timeout=30)
                    img_response_download.raise_for_status()
                    timestamp_img_url = datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')
                    if operation_type == "editing":
                        url_filename = f"edited_image_strategy_{strategy_index}.png"
                    else:
                        url_filename = f"generated_image_strategy_{strategy_index}.png"

                    saved_filepath = os.path.join(run_directory, url_filename)
                    with open(saved_filepath, "wb") as f:
                        for chunk in img_response_download.iter_content(chunk_size=8192): f.write(chunk)
                    print(f"   Saved downloaded image to: {saved_filepath}")
                    return "success", saved_filepath, prompt_tokens_for_image_gen
                except requests.exceptions.RequestException as req_err:
                    return "error", f"Error downloading image URL {image_url}: {req_err}", prompt_tokens_for_image_gen
            else:
                return "error", f"Image API response format mismatch for {operation_type}. No b64_json or URL.", prompt_tokens_for_image_gen
        else:
            return "error", "Image API response did not contain expected data structure.", prompt_tokens_for_image_gen

    except APIConnectionError as e:
        print(f"❌ ERROR: Image API connection error: {e}")
        return "error", f"Connection error: {e}", prompt_tokens_for_image_gen
    except RateLimitError as e:
        print(f"❌ ERROR: Image API rate limit exceeded: {e}")
        return "error", f"Rate limit error: {e}", prompt_tokens_for_image_gen
    except APIStatusError as e:
        print(f"❌ ERROR: Image API status error: {e.status_code} - {e.response}")
        error_message = f"API status error {e.status_code}"
        try:
            error_details = e.response.json()
            if 'error' in error_details and 'message' in error_details['error']:
                error_message += f": {error_details['error']['message']}"
        except:
            pass
        return "error", error_message, prompt_tokens_for_image_gen
    except Exception as e:
        print(f"❌ ERROR: Unexpected error during image {operation_type}: {e}")
        print(traceback.format_exc())
        return "error", f"Unexpected error: {e}", prompt_tokens_for_image_gen

print("✅ Core pipeline processing functions defined.")

# @title Step 7: Main Pipeline Orchestration Function (`run_full_pipeline`)

def _calculate_and_log_cost(
    pipeline_data: Dict[str, Any],
    stage_name: str,
    model_id: str,
    provider: str,
    duration: float, # Added duration
    usage_info: Optional[Dict[str, Any]] = None,
    image_details: Optional[Dict[str, Any]] = None
    # For image model: {"count": 1, "resolution": "1024x1024", "quality": "standard", "prompt_tokens": <int>}
):
    """Helper function to calculate cost for a stage and log it."""
    if not CostDetail or not PipelineCostSummary:
        print("Cost tracking models not defined. Skipping cost calculation.")
        return

    cost_summary_dict = pipeline_data["processing_context"].get("cost_summary")
    if cost_summary_dict is None: # Should have been initialized
        cost_summary_dict = PipelineCostSummary().model_dump()
        pipeline_data["processing_context"]["cost_summary"] = cost_summary_dict

    # Ensure stage_costs is a list
    if not isinstance(cost_summary_dict.get("stage_costs"), list):
        cost_summary_dict["stage_costs"] = []


    cost_data: Dict[str, Any] = {"stage_name": stage_name, "model_id": model_id, "provider": provider, "duration_seconds": round(duration, 3)}
    stage_total_cost = 0.0
    cost_calculation_notes = ""

    pricing = MODEL_PRICING.get(model_id)
    if not pricing:
        cost_calculation_notes = f"Pricing for model '{model_id}' not found. Cost not calculated."
        print(f"  ⚠️ {cost_calculation_notes}")
        cost_data["cost_calculation_notes"] = cost_calculation_notes
        cost_data["total_stage_cost_usd"] = 0.0
        cost_summary_dict["stage_costs"].append(CostDetail(**cost_data).model_dump())
        # No need to update total_pipeline_cost_usd if this stage cost is 0 or uncalculable
        return

    if usage_info and "input_cost_per_mtok" in pricing: # Text model
        input_tokens = usage_info.get("prompt_tokens", 0)
        output_tokens = usage_info.get("completion_tokens", 0)
        cost_data["input_tokens"] = input_tokens
        cost_data["output_tokens"] = output_tokens

        input_cost = (input_tokens / 1_000_000) * pricing["input_cost_per_mtok"]
        output_cost = (output_tokens / 1_000_000) * pricing["output_cost_per_mtok"]
        stage_total_cost = input_cost + output_cost

        cost_data["input_cost_usd"] = round(input_cost, 6)
        cost_data["output_cost_usd"] = round(output_cost, 6)
        cost_calculation_notes = pricing.get("notes", "")

    elif image_details and model_id == "gpt-image-1" and "cost_per_image" in pricing:
        num_images = image_details.get("count", 0) # Default to 0 if not successful
        resolution = image_details.get("resolution", "1024x1024")
        quality = image_details.get("quality", "medium").lower() # Default to medium
        prompt_tokens = image_details.get("prompt_tokens", 0) # Get prompt tokens for gpt-image-1

        cost_data["images_generated"] = num_images
        cost_data["resolution"] = resolution
        cost_data["quality"] = quality
        cost_data["input_tokens"] = prompt_tokens # Log prompt tokens for gpt-image-1

        image_output_cost = 0.0
        if num_images > 0: # Only calculate image output cost if an image was generated
            price_per_image = pricing.get("cost_per_image", {}).get(quality, {}).get(resolution)
            if price_per_image is not None:
                image_output_cost = num_images * price_per_image
            else:
                cost_calculation_notes += f" Pricing for gpt-image-1 with {quality}/{resolution} not found."

        input_text_cost = 0.0
        if "input_text_cost_per_mtok" in pricing and prompt_tokens > 0:
            input_text_cost = (prompt_tokens / 1_000_000) * pricing["input_text_cost_per_mtok"]
            cost_data["input_cost_usd"] = round(input_text_cost, 6)

        stage_total_cost = image_output_cost + input_text_cost
        cost_data["image_cost_usd"] = round(image_output_cost, 6) # Cost for the image output tokens/generation itself
        cost_calculation_notes += pricing.get("notes", "")
    else:
        cost_calculation_notes = "Usage info or image details missing/mismatched for cost calculation."
        print(f"  ⚠️ {cost_calculation_notes}")

    cost_data["total_stage_cost_usd"] = round(stage_total_cost, 6)
    cost_data["cost_calculation_notes"] = cost_calculation_notes.strip()

    cost_summary_dict["stage_costs"].append(CostDetail(**cost_data).model_dump())
    current_total_pipeline_cost = cost_summary_dict.get("total_pipeline_cost_usd", 0.0)
    cost_summary_dict["total_pipeline_cost_usd"] = round(current_total_pipeline_cost + stage_total_cost, 6)

    # Update pipeline_data directly
    pipeline_data["processing_context"]["cost_summary"] = cost_summary_dict

def run_full_pipeline():
    """Orchestrates the entire AI imagery pipeline."""
    overall_start_time = time.time() # For total pipeline latency
    with main_output_area: # All print statements will go here
        clear_output(wait=True)
        start_time_run = time.time() # Alias for clarity, same as overall_start_time
        print(f"--- Starting Full Pipeline Run: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ---")

        # 1. Create Run-Specific Output Directory
        run_timestamp_for_dir = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        current_run_dir = os.path.join(PIPELINE_OUTPUT_BASE_DIR, f"run_{run_timestamp_for_dir}")
        try:
            os.makedirs(current_run_dir, exist_ok=True)
            print(f"✅ Output directory for this run: {current_run_dir}")
        except Exception as e:
            print(f"❌ CRITICAL ERROR: Could not create run directory '{current_run_dir}': {e}. Halting pipeline.")
            return

        # 2. Gather Inputs
        print("\n--- Step 1: Gathering User Inputs ---")
        inputs_data = gather_all_inputs()
        mode = inputs_data['mode']
        # For display, create a copy and remove base64
        inputs_display = {k: v for k, v in inputs_data.items() if k != 'image_content_base64' and k != 'image_content_bytes'}
        if inputs_data.get('image_details'): inputs_display['image_details'] = inputs_data['image_details'] # Keep metadata
        print(f"  Inputs gathered for {mode}: {json.dumps(inputs_display, indent=2, default=str)}")


        # 3. Client-Side Input Validation
        print("\n--- Step 2: Client-Side Input Validation ---")
        is_valid, validation_msg = validate_inputs(mode, inputs_data)
        print(f"  Status: {validation_msg}")
        if not is_valid:
            print(f"--- Pipeline ENDED (Client-Side Validation Failed) ---")
            return

        # 4. Generate Initial Pipeline Data Structure
        print("\n--- Step 3: Generating Initial Pipeline Data Structure ---")
        pipeline_data = generate_initial_pipeline_data(mode, inputs_data)
        # Initialize cost summary if not already done by generate_initial_pipeline_data
        if "cost_summary" not in pipeline_data["processing_context"] or pipeline_data["processing_context"]["cost_summary"] is None:
            pipeline_data["processing_context"]["cost_summary"] = PipelineCostSummary().model_dump() if PipelineCostSummary else None

        # Display sanitized version (without base64)
        pd_display = json.loads(json.dumps(pipeline_data, default=str)) # Deep copy for display
        if pd_display.get("user_inputs",{}).get("image_reference",{}).get("image_content_base64"):
            pd_display["user_inputs"]["image_reference"]["image_content_base64"] = "[[Base64 Content Removed for Final Save]]"
        print(json.dumps(pd_display, indent=2))

        # 5. Server-Side JSON Validation (Simulated)
        print("\n--- Step 4: Server-Side Pipeline Data Validation ---")
        data_ok, data_validation_msg = parse_and_validate_pipeline_data(pipeline_data)
        print(f"  Status: {data_validation_msg}")
        if not data_ok:
            print(f"--- Pipeline ENDED (Server-Side Validation Failed) ---")
            # Save the invalid data for debugging
            with open(os.path.join(current_run_dir, "INVALID_pipeline_data_initial.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return

        # 6. Save Original Input Image (if any) to run directory
        # This path will be used by generate_image if an edit is performed
        if inputs_data.get('image_content_bytes') and pipeline_data.get("user_inputs", {}).get("image_reference"):
            try:
                img_ref = pipeline_data["user_inputs"]["image_reference"]
                original_filename = img_ref.get("filename", "uploaded_image.jpg")
                # Sanitize filename for saving
                safe_original_filename = "".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in original_filename)
                # Force JPEG extension for consistency if desired, or try to keep original
                # For simplicity, let's try to keep original extension or default to .jpg
                base_name, ext = os.path.splitext(safe_original_filename)
                if not ext: ext = ".jpg" # Default extension
                saved_input_image_filename = f"input_{base_name}_{pipeline_data['pipeline_settings']['run_timestamp']}{ext}"

                saved_input_image_path_in_run_dir = os.path.join(current_run_dir, saved_input_image_filename)

                img_to_save = Image.open(io.BytesIO(inputs_data['image_content_bytes']))
                # Convert to RGB if it has alpha, to ensure JPEG compatibility if saving as JPEG
                if img_to_save.mode in ('RGBA', 'LA', 'P'):
                    background = Image.new('RGB', img_to_save.size, (255, 255, 255))
                    background.paste(img_to_save, (0, 0), img_to_save.split()[-1] if len(img_to_save.split()) == 4 else None)
                    img_to_save = background
                elif img_to_save.mode != 'RGB':
                    img_to_save = img_to_save.convert('RGB')

                img_to_save.save(saved_input_image_path_in_run_dir) # Saves in original format or converts if needed by Pillow
                print(f"  Input image saved to: {saved_input_image_path_in_run_dir}")
                # Update pipeline_data with the path within the run directory
                pipeline_data["user_inputs"]["image_reference"]["saved_image_path_in_run_dir"] = saved_input_image_path_in_run_dir
            except Exception as img_save_err:
                print(f"  ERROR saving input image: {img_save_err}")
                if pipeline_data.get("user_inputs", {}).get("image_reference"):
                    pipeline_data["user_inputs"]["image_reference"]["saved_image_path_in_run_dir"] = None
        else: # Ensure the path is None if no image was uploaded/processed
            if pipeline_data.get("user_inputs", {}).get("image_reference"):
                pipeline_data["user_inputs"]["image_reference"]["saved_image_path_in_run_dir"] = None


        # 7. Perform Image Evaluation
        stage_start_time = time.time()
        print("\n--- Step 5: Image Evaluation ---")
        eval_status_msg, _, eval_usage_info, eval_status_code = perform_image_evaluation(pipeline_data)
        stage_duration = time.time() - stage_start_time
        print(f"  Status: {eval_status_msg} (Duration: {stage_duration:.2f}s)")
        if eval_usage_info:
            _calculate_and_log_cost(pipeline_data, "Image Evaluation", IMG_EVAL_MODEL_ID, IMG_EVAL_MODEL_PROVIDER, duration=stage_duration, usage_info=eval_usage_info)
        # Display analysis result (if any)
        if pipeline_data["processing_context"]["image_analysis_result"]:
            print("  Image Analysis Result:")
            print(json.dumps(pipeline_data["processing_context"]["image_analysis_result"], indent=2))
        if eval_status_code == 'API_ERROR' or eval_status_code == 'API_ERROR_GENERAL': # Halt on critical API failure
            print(f"--- Pipeline ENDED (Image Evaluation API Failed) ---")
            with open(os.path.join(current_run_dir, "ERROR_pipeline_data_image_eval.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return

        # 8. Generate Marketing Strategies
        stage_start_time = time.time()
        print("\n--- Step 6: Marketing Strategy Generation ---")
        num_strategies_to_gen = 3 # Or make this a user input
        strat_status_msg, _, strat_usage_info, strat_status_code = generate_marketing_strategies(pipeline_data, num_strategies=num_strategies_to_gen)
        stage_duration = time.time() - stage_start_time
        print(f"  Status: {strat_status_msg} (Duration: {stage_duration:.2f}s)")
        # Cost for sub-stages (Niche ID and Goal Gen) are logged within generate_marketing_strategies
        # if strat_usage_info: # This usage info is combined from niche and goal gen
        #      # Log Niche ID cost
        #     if pipeline_data["processing_context"]["llm_call_usage"].get("strategy_niche_id"):
        #         _calculate_and_log_cost(pipeline_data, "Strategy - Niche ID", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER, duration=stage_duration,
        #                                 usage_info=pipeline_data["processing_context"]["llm_call_usage"]["strategy_niche_id"])
        #     # Log Goal Gen cost
        #     if pipeline_data["processing_context"]["llm_call_usage"].get("strategy_goal_gen"):
        #         _calculate_and_log_cost(pipeline_data, "Strategy - Goal Gen", STRATEGY_MODEL_ID, STRATEGY_MODEL_PROVIDER, duration=stage_duration,
        #                                 usage_info=pipeline_data["processing_context"]["llm_call_usage"]["strategy_goal_gen"])
        if pipeline_data["processing_context"]["suggested_marketing_strategies"]:
            print(f"  Generated {len(pipeline_data['processing_context']['suggested_marketing_strategies'])} Marketing Strategies:")
            for i, strat in enumerate(pipeline_data["processing_context"]["suggested_marketing_strategies"]):
                print(f"  Strategy {i}: {json.dumps(strat, indent=2)}")
        if strat_status_code == 'API_ERROR': # Halt on critical API failure
            print(f"--- Pipeline ENDED (Marketing Strategy Generation API Failed) ---")
            with open(os.path.join(current_run_dir, "ERROR_pipeline_data_mkt_strat.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return

        # 9. Generate Style Guidance
        stage_start_time = time.time()
        print("\n--- Step 7: Style Guidance Generation ---")
        style_guidance_list, style_guide_usage, style_guide_error = generate_style_guidance_for_strategies(pipeline_data)
        stage_duration = time.time() - stage_start_time
        if style_guide_usage:
            _calculate_and_log_cost(pipeline_data, "Style Guidance", STYLE_GUIDER_MODEL_ID, STYLE_GUIDER_MODEL_PROVIDER, duration=stage_duration, usage_info=style_guide_usage)
        if style_guide_error or not style_guidance_list:
            print(f"  ERROR: Style Guider Agent failed: {style_guide_error} (Duration: {stage_duration:.2f}s)")
            print(f"--- Pipeline ENDED (Style Guidance Failed) ---")
            with open(os.path.join(current_run_dir, "ERROR_pipeline_data_style_guide.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return
        else:
            # pipeline_data["processing_context"]["style_guidance_sets"] is updated inside the function
            print(f"  Successfully generated {len(style_guidance_list)} style guidance sets (Duration: {stage_duration:.2f}s):")
            for i, sg in enumerate(style_guidance_list):
                 print(f"  Style Guidance {i}: {json.dumps(sg, indent=2)}")


        # 10. Generate Structured Visual Concepts (Creative Expert)
        print("\n--- Step 8: Creative Visual Concept Generation ---")
        strategies_for_creative = pipeline_data.get("processing_context", {}).get("suggested_marketing_strategies", [])
        style_guidance_for_creative = pipeline_data.get("processing_context", {}).get("style_guidance_sets", [])
        generated_prompts_list = [] # Store ImageGenerationPrompt dicts

        if not strategies_for_creative or not style_guidance_for_creative or len(strategies_for_creative) != len(style_guidance_for_creative):
            print("  ERROR: Mismatch or missing strategies/style guidance. Cannot proceed with concept generation.")
            print(f"--- Pipeline ENDED (Data Mismatch for Creative Expert) ---")
            with open(os.path.join(current_run_dir, "ERROR_pipeline_data_creative_mismatch.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return

        all_concepts_generated = True
        for idx, strategy_item in enumerate(strategies_for_creative):
            stage_start_time_ce = time.time()
            style_item_dict = style_guidance_for_creative[idx]
            try: # Ensure style_item_dict can be parsed into StyleGuidance
                style_item_pydantic = StyleGuidance(**style_item_dict) if StyleGuidance else style_item_dict # type: ignore
            except ValidationError as ve:
                print(f"  ERROR: Invalid style guidance format for strategy {idx}: {ve}. Skipping concept generation for this strategy.")
                all_concepts_generated = False
                continue

            concept_dict, concept_usage, concept_error = generate_image_prompt_for_strategy(
                pipeline_data, strategy_item, idx, style_item_pydantic
            )
            stage_duration_ce = time.time() - stage_start_time_ce
            if concept_usage:
                 _calculate_and_log_cost(pipeline_data, f"Creative Expert - Strategy {idx}", CREATIVE_EXPERT_MODEL_ID, CREATIVE_EXPERT_MODEL_PROVIDER, duration=stage_duration_ce, usage_info=concept_usage)
                 # Store usage in llm_call_usage as well
                 pipeline_data["processing_context"]["llm_call_usage"][f"creative_expert_strategy_{idx}"] = concept_usage
            if concept_error or not concept_dict:
                print(f"  ERROR generating visual concept for Strategy {idx}: {concept_error} (Duration: {stage_duration_ce:.2f}s)")
                all_concepts_generated = False
                # Optionally break or continue based on desired error handling
            else:
                generated_prompts_list.append(concept_dict)
                print(f"  Visual Concept for Strategy {idx} (Source Strategy Index: {concept_dict.get('source_strategy_index')} ) (Duration: {stage_duration_ce:.2f}s):")
                # Display VisualConceptDetails without too much nesting
                vc_display = concept_dict.get("visual_concept", {})
                print(json.dumps(vc_display, indent=2))

        pipeline_data["processing_context"]["generated_image_prompts"] = generated_prompts_list
        if not all_concepts_generated and not generated_prompts_list: # Total failure
            print(f"--- Pipeline ENDED (No Visual Concepts Generated) ---")
            with open(os.path.join(current_run_dir, "ERROR_pipeline_data_no_concepts.json"), 'w') as f: json.dump(pipeline_data, f, indent=2, default=str)
            return
        elif not all_concepts_generated:
            print("  WARNING: One or more visual concepts failed to generate.")


        # 11. Assemble Final Prompts and Generate Images
        print("\n--- Step 9: Assembling Final Prompts & Generating Images ---")
        structured_prompts_for_assembly = pipeline_data.get("processing_context", {}).get("generated_image_prompts", [])
        final_assembled_prompts_list = []
        generated_image_results_list = []
        platform_aspect_ratio_for_img_gen = pipeline_data.get("request_details", {}).get("target_platform", {}).get("resolution_details", {}).get("aspect_ratio", "1:1")
        # Get the saved reference image path from pipeline_data
        reference_image_path_for_gen = None
        if pipeline_data.get("user_inputs", {}).get("image_reference"):
            reference_image_path_for_gen = pipeline_data["user_inputs"]["image_reference"].get("saved_image_path_in_run_dir")

        if not image_gen_client:
            print("  ERROR: Image Generation Client not configured. Skipping image generation.")
        elif not structured_prompts_for_assembly:
            print("  No structured prompts available to generate images from.")
        else:
            for struct_prompt in structured_prompts_for_assembly:
                stage_start_time_img = time.time()
                s_idx = struct_prompt.get("source_strategy_index", "N/A")
                print(f"\n  Processing for Strategy {s_idx}:")
                final_text_prompt = assemble_final_prompt(struct_prompt, pipeline_data["user_inputs"], platform_aspect_ratio_for_img_gen)
                final_assembled_prompts_list.append({"index": s_idx, "prompt": final_text_prompt})
                print(f"    Assembled Final Prompt (Strategy {s_idx}): {final_text_prompt[:300]}...") # Show snippet

                if final_text_prompt.startswith("Error:"):
                    print(f"    Skipping image generation due to prompt assembly error: {final_text_prompt}")
                    generated_image_results_list.append({"index": s_idx, "status": "error", "result_path": None, "error_message": final_text_prompt})
                    stage_duration_img = time.time() - stage_start_time_img
                    _calculate_and_log_cost(pipeline_data, f"Image Generation - Strategy {s_idx}",
                                            IMAGE_GENERATION_MODEL_ID,
                                            MODEL_PRICING.get(IMAGE_GENERATION_MODEL_ID, {}).get("provider", "OpenAI"),
                                            duration=stage_duration_img,
                                            image_details={"count": 0, "error": final_text_prompt, "prompt_tokens": len(final_text_prompt.split())})
                    continue

                img_status, img_result_path_or_msg, img_prompt_tokens = generate_image(
                    final_text_prompt,
                    platform_aspect_ratio_for_img_gen,
                    image_gen_client,
                    current_run_dir,
                    s_idx,
                    reference_image_path=reference_image_path_for_gen,
                    image_quality_setting="medium" # Default for gpt-image-1
                )
                stage_duration_img = time.time() - stage_start_time_img
                img_api_size = map_aspect_ratio_to_size_for_api(platform_aspect_ratio_for_img_gen)
                image_gen_quality_for_cost = "medium" # Default for gpt-image-1

                if img_status == "success":
                    generated_image_results_list.append({"index": s_idx, "status": "success", "result_path": img_result_path_or_msg, "error_message": None})
                    print(f"    Image generated/edited successfully: {img_result_path_or_msg} (Duration: {stage_duration_img:.2f}s)")

                    _calculate_and_log_cost(pipeline_data, f"Image Generation - Strategy {s_idx}",
                                            IMAGE_GENERATION_MODEL_ID,
                                            MODEL_PRICING.get(IMAGE_GENERATION_MODEL_ID, {}).get("provider", "OpenAI"),
                                            duration=stage_duration_img,
                                            image_details={"count": 1, "resolution": img_api_size, "quality": image_gen_quality_for_cost, "prompt_tokens": img_prompt_tokens})
                    try: display(IPImage(filename=img_result_path_or_msg, width=256)) # type: ignore
                    except Exception as disp_e: print(f"    Error displaying image: {disp_e}")
                else:
                    generated_image_results_list.append({"index": s_idx, "status": "error", "result_path": None, "error_message": img_result_path_or_msg})
                    print(f"    Image generation/editing failed: {img_result_path_or_msg} (Duration: {stage_duration_img:.2f}s)")
                    _calculate_and_log_cost(pipeline_data, f"Image Generation - Strategy {s_idx}",
                                        IMAGE_GENERATION_MODEL_ID,
                                        MODEL_PRICING.get(IMAGE_GENERATION_MODEL_ID, {}).get("provider", "OpenAI"),
                                        duration=stage_duration_img,
                                        image_details={"count": 0, "error": img_result_path_or_msg, "resolution": img_api_size, "quality": image_gen_quality_for_cost, "prompt_tokens": img_prompt_tokens})

        pipeline_data["processing_context"]["final_assembled_prompts"] = final_assembled_prompts_list
        pipeline_data["processing_context"]["generated_image_results"] = generated_image_results_list

        # 12. Final Summary and Saving
        print("\n--- Step 10: Final Summary & Saving ---")

        # Display Cost Summary
        overall_pipeline_duration = time.time() - overall_start_time
        if pipeline_data["processing_context"].get("cost_summary") and PipelineCostSummary:
            cost_summary_obj_for_total_duration = PipelineCostSummary(**pipeline_data["processing_context"]["cost_summary"]) # type: ignore
            cost_summary_obj_for_total_duration.total_pipeline_duration_seconds = round(overall_pipeline_duration, 2)
            pipeline_data["processing_context"]["cost_summary"] = cost_summary_obj_for_total_duration.model_dump()

        cost_summary = pipeline_data["processing_context"].get("cost_summary")
        if cost_summary and CostDetail and PipelineCostSummary: # Check if Pydantic models are available
            print("\n  --- Estimated Cost & Latency Summary ---")
            # Re-parse for easy iteration if it was dumped
            cost_summary_obj = PipelineCostSummary(**cost_summary)

            if VERBOSE_COST_LATENCY_SUMMARY:
                for stage_cost_item in cost_summary_obj.stage_costs: # Iterate directly over CostDetail objects
                    # stage_cost_item is already a CostDetail object
                    print(f"    Stage: {stage_cost_item.stage_name} (Model: {stage_cost_item.model_id} via {stage_cost_item.provider})")
                    if stage_cost_item.duration_seconds is not None:
                        print(f"      Duration: {stage_cost_item.duration_seconds:.2f}s")
                    if stage_cost_item.input_tokens is not None and stage_cost_item.output_tokens is not None: # Check both for text models
                        print(f"      Tokens: Input={stage_cost_item.input_tokens}, Output={stage_cost_item.output_tokens}")
                        print(f"      Cost: Input=${stage_cost_item.input_cost_usd:.6f}, Output=${stage_cost_item.output_cost_usd:.6f}")
                    elif stage_cost_item.images_generated is not None:
                        print(f"      Images: {stage_cost_item.images_generated}, Resolution: {stage_cost_item.resolution}, Quality: {stage_cost_item.quality or 'N/A'}")
                        if stage_cost_item.input_tokens is not None and stage_cost_item.input_cost_usd is not None: # For gpt-image-1 text prompt tokens
                            print(f"      Input Text Tokens: {stage_cost_item.input_tokens}, Cost: ${stage_cost_item.input_cost_usd:.6f}")
                        if stage_cost_item.image_cost_usd is not None:
                            print(f"      Image Output Cost: ${stage_cost_item.image_cost_usd:.6f}")
                    print(f"      Stage Total Cost: ${stage_cost_item.total_stage_cost_usd:.6f}")
                    if stage_cost_item.cost_calculation_notes:
                        print(f"      Notes: {stage_cost_item.cost_calculation_notes}")

            print(f"  ------------------------------------")
            print(f"  Total Estimated Pipeline Cost: ${cost_summary_obj.total_pipeline_cost_usd:.6f} {cost_summary_obj.currency}")
            print(f"  Total Pipeline Duration: {cost_summary_obj.total_pipeline_duration_seconds:.2f}s")
            if VERBOSE_COST_LATENCY_SUMMARY:
                print(f"  {cost_summary_obj.pricing_info_url}")
                if cost_summary_obj.cost_calculation_error:
                    print(f"  Cost Calculation Error: {cost_summary_obj.cost_calculation_error}")
            else:
                print(f"  (Set VERBOSE_COST_LATENCY_SUMMARY=True in Step 1 for detailed breakdown)")

        else:
            print("  Cost summary not available or Pydantic models for cost tracking failed to load.")

        # Aggregate LLM usage (already being done, but can be part of the summary print)
        print("\n  LLM Call Token Usage Summary (Raw):")
        # Aggregate LLM usage
        total_tokens_all_calls = 0

        for call_name, usage_data in pipeline_data["processing_context"]["llm_call_usage"].items():
            if usage_data and isinstance(usage_data, dict): # Check if usage_data is not None and is a dict
                call_total = usage_data.get("total_tokens", 0)
                total_tokens_all_calls += call_total
                print(f"    {call_name}: Prompt={usage_data.get('prompt_tokens',0)}, Completion={usage_data.get('completion_tokens',0)}, Total={call_total}")
        print(f"  Total Tokens for all LLM calls in this run (from usage objects): {total_tokens_all_calls}")

        # Save the final comprehensive pipeline_data JSON
        final_json_path = os.path.join(current_run_dir, f"pipeline_run_metadata_{run_timestamp_for_dir}.json")
        try:
            # Create a display copy and sanitize base64 before saving
            pipeline_data_to_save = json.loads(json.dumps(pipeline_data, default=str))
            if pipeline_data_to_save.get("user_inputs",{}).get("image_reference",{}).get("image_content_base64"):
                pipeline_data_to_save["user_inputs"]["image_reference"]["image_content_base64"] = "[[Base64 Content Removed for Final Save]]"

            with open(final_json_path, 'w') as f:
                json.dump(pipeline_data_to_save, f, indent=2)
            print(f"\n✅ Final pipeline metadata saved to: {final_json_path}")
        except Exception as save_err:
            print(f"❌ Error saving final pipeline metadata: {save_err}")

        end_time = time.time() # Redundant, overall_pipeline_duration already calculated
        print(f"\n--- Pipeline Run FINISHED in {overall_pipeline_duration:.2f} seconds ---")
        print(f"All outputs for this run are in: {current_run_dir}")

# @title Step 8: Run Full AI Imagery Pipeline
# --- Instructions ---
# 1. Configure your API keys in the .env file (refer to Step 1).
# 2. Select the desired mode and fill in the inputs in Step 3 UI.
# 3. Run THIS cell to execute the entire pipeline.
# Outputs (images, metadata JSON) will be saved to a new timestamped folder
# inside 'pipeline_runs' in your Google Drive (AI Imagery Marketing Tool/Colab Notebook/pipeline_runs).

# Ensure the main_output_area is displayed in this cell's output
display(main_output_area)

run_full_pipeline()
# The `main_output_area` widget is used internally by `run_full_pipeline`.
# To re-display the UI if needed after a run, uncomment and run the display line from Step 3:
# display(mode_selection, input_container)

